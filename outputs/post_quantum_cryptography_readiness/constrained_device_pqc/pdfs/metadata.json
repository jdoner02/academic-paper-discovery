{
  "download_info": {
    "timestamp": "2025-08-07T16:24:47.639131+00:00",
    "strategy_name": "Post-Quantum Cryptography for Constrained Devices",
    "total_papers": 50,
    "download_directory": "outputs/post_quantum_cryptography_readiness/constrained_device_pqc/pdfs"
  },
  "papers": [
    {
      "title": "Evaluating Post-Quantum Cryptographic Algorithms on Resource-Constrained   Devices",
      "authors": [
        "Jesus Lopez",
        "Viviana Cadena",
        "Mohammad Saidur Rahman"
      ],
      "abstract": "The rapid advancement of quantum computing poses a critical threat to classical cryptographic algorithms such as RSA and ECC, particularly in Internet of Things (IoT) devices, where secure communication is essential but often constrained by limited computational resources. This paper investigates the feasibility of deploying post-quantum cryptography (PQC) algorithms on resource-constrained devices. In particular, we implement three PQC algorithms -- BIKE, CRYSTALS-Kyber, and HQC -- on a lightweight IoT platform built with Raspberry Pi devices. Leveraging the Open Quantum Safe (\\texttt{liboqs}) library in conjunction with \\texttt{mbedTLS}, we develop quantum-secure key exchange protocols, and evaluate their performance in terms of computational overhead, memory usage, and energy consumption for quantum secure communication. Experimental results demonstrate that the integration of PQC algorithms on constrained hardware is practical, reinforcing the urgent need for quantum-resilient cryptographic frameworks in next-generation IoT devices. The implementation of this paper is available at https://iqsec-lab.github.io/PQC-IoT/.",
      "publication_date": "2025-07-11T05:03:19+00:00",
      "doi": "",
      "venue": "arXiv preprint",
      "citation_count": 0,
      "keywords": [
        "cs.CR"
      ],
      "arxiv_id": "2507.08312v1"
    },
    {
      "title": "PQ-CAN: A Framework for Simulating Post-Quantum Cryptography in Embedded   Systems",
      "authors": [
        "Mauro Conti",
        "Francesco Marchiori",
        "Sebastiano Matarazzo",
        "Marco Rubin"
      ],
      "abstract": "The rapid development of quantum computers threatens traditional cryptographic schemes, prompting the need for Post-Quantum Cryptography (PQC). Although the NIST standardization process has accelerated the development of such algorithms, their application in resource-constrained environments such as embedded systems remains a challenge. Automotive systems relying on the Controller Area Network (CAN) bus for communication are particularly vulnerable due to their limited computational capabilities, high traffic, and need for real-time response. These constraints raise concerns about the feasibility of implementing PQC in automotive environments, where legacy hardware and bit rate limitations must also be considered.   In this paper, we introduce PQ-CAN, a modular framework for simulating the performance and overhead of PQC algorithms in embedded systems. We consider the automotive domain as our case study, testing a variety of PQC schemes under different scenarios. Our simulation enables the adjustment of embedded system computational capabilities and CAN bus bit rate constraints. We also provide insights into the trade-offs involved by analyzing each algorithm's security level and overhead for key encapsulation and digital signature. By evaluating the performance of these algorithms, we provide insights into their feasibility and identify the strengths and limitations of PQC in securing automotive communications in the post-quantum era.",
      "publication_date": "2025-04-14T21:50:26+00:00",
      "doi": "",
      "venue": "arXiv preprint",
      "citation_count": 0,
      "keywords": [
        "cs.CR"
      ],
      "arxiv_id": "2504.10730v1"
    },
    {
      "title": "A Systematic Study of Lattice-based NIST PQC Algorithms: from Reference   Implementations to Hardware Accelerators",
      "authors": [
        "Malik Imran",
        "Zain Ul Abideen",
        "Samuel Pagliarini"
      ],
      "abstract": "Security of currently deployed public key cryptography algorithms is foreseen to be vulnerable against quantum computer attacks. Hence, a community effort exists to develop post-quantum cryptography (PQC) algorithms, i.e., algorithms that are resistant to quantum attacks. In this work, we have investigated how lattice-based candidate algorithms from the NIST PQC standardization competition fare when conceived as hardware accelerators. To achieve this, we have assessed the reference implementations of selected algorithms with the goal of identifying what are their basic building blocks. We assume the hardware accelerators will be implemented in application specific integrated circuit (ASIC) and the targeted technology in our experiments is a commercial 65nm node. In order to estimate the characteristics of each algorithm, we have assessed their memory requirements, use of multipliers, and how each algorithm employs hashing functions. Furthermore, for these building blocks, we have collected area and power figures for 12 candidate algorithms. For memories, we make use of a commercial memory compiler. For logic, we make use of a standard cell library. In order to compare the candidate algorithms fairly, we select a reference frequency of operation of 500MHz. Our results reveal that our area and power numbers are comparable to the state of the art, despite targeting a higher frequency of operation and a higher security level in our experiments. The comprehensive investigation of lattice-based NIST PQC algorithms performed in this paper can be used for guiding ASIC designers when selecting an appropriate algorithm while respecting requirements and design constraints.",
      "publication_date": "2020-09-15T13:32:03+00:00",
      "doi": "10.3390/electronics9111953",
      "venue": "arXiv preprint",
      "citation_count": 0,
      "keywords": [
        "cs.CR"
      ],
      "arxiv_id": "2009.07091v3"
    },
    {
      "title": "Complexity of Post-Quantum Cryptography in Embedded Systems and Its   Optimization Strategies",
      "authors": [
        "Omar Alnaseri",
        "Yassine Himeur",
        "Shadi Atalla",
        "Wathiq Mansoor"
      ],
      "abstract": "With the rapid advancements in quantum computing, traditional cryptographic schemes like Rivest-Shamir-Adleman (RSA) and elliptic curve cryptography (ECC) are becoming vulnerable, necessitating the development of quantum-resistant algorithms. The National Institute of Standards and Technology (NIST) has initiated a standardization process for PQC algorithms, and several candidates, including CRYSTALS-Kyber and McEliece, have reached the final stages. This paper first provides a comprehensive analysis of the hardware complexity of post-quantum cryptography (PQC) in embedded systems, categorizing PQC algorithms into families based on their underlying mathematical problems: lattice-based, code-based, hash-based and multivariate / isogeny-based schemes. Each family presents distinct computational, memory, and energy profiles, making them suitable for different use cases. To address these challenges, this paper discusses optimization strategies such as pipelining, parallelization, and high-level synthesis (HLS), which can improve the performance and energy efficiency of PQC implementations. Finally, a detailed complexity analysis of CRYSTALS-Kyber and McEliece, comparing their key generation, encryption, and decryption processes in terms of computational complexity, has been conducted.",
      "publication_date": "2025-04-18T08:02:13+00:00",
      "doi": "",
      "venue": "arXiv preprint",
      "citation_count": 0,
      "keywords": [
        "cs.CR"
      ],
      "arxiv_id": "2504.13537v1"
    },
    {
      "title": "AES-RV: Hardware-Efficient RISC-V Accelerator with Low-Latency AES   Instruction Extension for IoT Security",
      "authors": [
        "Van Tinh Nguyen",
        "Phuc Hung Pham",
        "Vu Trung Duong Le",
        "Hoai Luan Pham",
        "Tuan Hai Vu",
        "Thi Diem Tran"
      ],
      "abstract": "The Advanced Encryption Standard (AES) is a widely adopted cryptographic algorithm essential for securing embedded systems and IoT platforms. However, existing AES hardware accelerators often face limitations in performance, energy efficiency, and flexibility. This paper presents AES-RV, a hardware-efficient RISC-V accelerator featuring low-latency AES instruction extensions optimized for real-time processing across all AES modes and key sizes. AES-RV integrates three key innovations: high-bandwidth internal buffers for continuous data processing, a specialized AES unit with custom low-latency instructions, and a pipelined system supported by a ping-pong memory transfer mechanism. Implemented on the Xilinx ZCU102 SoC FPGA, AES-RV achieves up to 255.97 times speedup and up to 453.04 times higher energy efficiency compared to baseline and conventional CPU/GPU platforms. It also demonstrates superior throughput and area efficiency against state-of-the-art AES accelerators, making it a strong candidate for secure and high-performance embedded systems.",
      "publication_date": "2025-05-17T07:15:36+00:00",
      "doi": "",
      "venue": "arXiv preprint",
      "citation_count": 0,
      "keywords": [
        "cs.AR"
      ],
      "arxiv_id": "2505.11880v1"
    },
    {
      "title": "FPGA-Optimized Hardware Accelerator for Fast Fourier Transform and   Singular Value Decomposition in AI",
      "authors": [
        "Hong Ding",
        "Chia Chao Kang",
        "SuYang Xi",
        "Zehang Liu",
        "Xuan Zhang",
        "Yi Ding"
      ],
      "abstract": "This research introduces an FPGA-based hardware accelerator to optimize the Singular Value Decomposition (SVD) and Fast Fourier transform (FFT) operations in AI models. The proposed design aims to improve processing speed and reduce computational latency. Through experiments, we validate the performance benefits of the hardware accelerator and show how well it handles FFT and SVD operations. With its strong security and durability, the accelerator design achieves significant speedups over software implementations, thanks to its modules for data flow control, watermark embedding, FFT, and SVD.",
      "publication_date": "2025-04-14T16:58:46+00:00",
      "doi": "",
      "venue": "arXiv preprint",
      "citation_count": 0,
      "keywords": [
        "cs.AR"
      ],
      "arxiv_id": "2504.10411v1"
    },
    {
      "title": "PLS-Assisted Offloading for Edge Computing-Enabled Post-Quantum Security   in Resource-Constrained Devices",
      "authors": [
        "Hamid Amiriara",
        "Mahtab Mirmohseni",
        "Rahim Tafazolli"
      ],
      "abstract": "With the advent of post-quantum cryptography (PQC) standards, it has become imperative for resource-constrained devices (RCDs) in the Internet of Things (IoT) to adopt these quantum-resistant protocols. However, the high computational overhead and the large key sizes associated with PQC make direct deployment on such devices impractical. To address this challenge, we propose an edge computing-enabled PQC framework that leverages a physical-layer security (PLS)-assisted offloading strategy, allowing devices to either offload intensive cryptographic tasks to a post-quantum edge server (PQES) or perform them locally. Furthermore, to ensure data confidentiality within the edge domain, our framework integrates two PLS techniques: offloading RCDs employ wiretap coding to secure data transmission, while non-offloading RCDs serve as friendly jammers by broadcasting artificial noise to disrupt potential eavesdroppers. Accordingly, we co-design the computation offloading and PLS strategy by jointly optimizing the device transmit power, PQES computation resource allocation, and offloading decisions to minimize overall latency under resource constraints. Numerical results demonstrate significant latency reductions compared to baseline schemes, confirming the scalability and efficiency of our approach for secure PQC operations in IoT networks.",
      "publication_date": "2025-04-13T05:14:17+00:00",
      "doi": "",
      "venue": "arXiv preprint",
      "citation_count": 0,
      "keywords": [
        "cs.CR"
      ],
      "arxiv_id": "2504.09437v1"
    },
    {
      "title": "FPGA Hardware Acceleration for Feature-Based Relative Navigation   Applications",
      "authors": [
        "Ramchander Rao Bhaskara",
        "Manoranjan Majji"
      ],
      "abstract": "Estimation of rigid transformation between two point clouds is a computationally challenging problem in vision-based relative navigation. Targeting a real-time navigation solution utilizing point-cloud and image registration algorithms, this paper develops high-performance avionics for power and resource constrained pose estimation framework. A Field-Programmable Gate Array (FPGA) based embedded architecture is developed to accelerate estimation of relative pose between the point-clouds, aided by image features that correspond to the individual point sets. At algorithmic level, the pose estimation method is an adaptation of Optimal Linear Attitude and Translation Estimator (OLTAE) for relative attitude and translation estimation. At the architecture level, the proposed embedded solution is a hardware/software co-design that evaluates the OLTAE computations on the bare-metal hardware for high-speed state estimation. The finite precision FPGA evaluation of the OLTAE algorithm is compared with a double-precision evaluation on MATLAB for performance analysis and error quantification. Implementation results of the proposed finite-precision OLTAE accelerator demonstrate the high-performance compute capabilities of the FPGA-based pose estimation while offering relative numerical errors below 7%.",
      "publication_date": "2022-10-18T00:01:57+00:00",
      "doi": "",
      "venue": "arXiv preprint",
      "citation_count": 0,
      "keywords": [
        "cs.RO"
      ],
      "arxiv_id": "2210.09481v1"
    },
    {
      "title": "Rudraksh: A compact and lightweight post-quantum key-encapsulation   mechanism",
      "authors": [
        "Suparna Kundu",
        "Archisman Ghosh",
        "Angshuman Karmakar",
        "Shreyas Sen",
        "Ingrid Verbauwhede"
      ],
      "abstract": "Resource-constrained devices such as wireless sensors and Internet of Things (IoT) devices have become ubiquitous in our digital ecosystem. These devices generate and handle a major part of our digital data. However, due to the impending threat of quantum computers on our existing public-key cryptographic schemes and the limited resources available on IoT devices, it is important to design lightweight post-quantum cryptographic (PQC) schemes suitable for these devices. In this work, we explored the design space of learning with error-based PQC schemes to design a lightweight key-encapsulation mechanism (KEM) suitable for resource-constrained devices. We have done a scrupulous and extensive analysis and evaluation of different design elements, such as polynomial size, field modulus structure, reduction algorithm, and secret and error distribution of an LWE-based KEM. Our explorations led to the proposal of a lightweight PQC-KEM, Rudraksh, without compromising security. Our scheme provides security against chosen ciphertext attacks (CCA) with more than 100 bits of Core-SVP post-quantum security and belongs to the NIST-level-I security category (provide security at least as much as AES-128). We have also shown how ASCON can be used for lightweight pseudo-random number generation and hash function in the lattice-based KEMs instead of the widely used Keccak for lightweight design. Our FPGA results show that Rudraksh currently requires the least area among the PQC KEMs of similar security. Our implementation of Rudraksh provides a $\\sim3\\times$ improvement in terms of the area requirement compared to the state-of-the-art area-optimized implementation of Kyber, can operate at $63\\%$-$76\\%$ higher frequency with respect to high-throughput Kyber, and improves time-area-product $\\sim2\\times$ compared to the state-of-the-art compact implementation of Kyber published in HPEC 2022.",
      "publication_date": "2025-01-23T16:16:23+00:00",
      "doi": "",
      "venue": "arXiv preprint",
      "citation_count": 0,
      "keywords": [
        "cs.CR"
      ],
      "arxiv_id": "2501.13799v1"
    },
    {
      "title": "Computation offloading to hardware accelerators in Intel SGX and Gramine   Library OS",
      "authors": [
        "Dmitrii Kuvaiskii",
        "Gaurav Kumar",
        "Mona Vij"
      ],
      "abstract": "The Intel Software Guard Extensions (SGX) technology enables applications to run in an isolated SGX enclave environment, with elevated confidentiality and integrity guarantees. Gramine Library OS facilitates execution of existing unmodified applications in SGX enclaves, requiring only an accompanying manifest file that describes the application's security posture and configuration. However, Intel SGX is a CPU-only technology, thus Gramine currently supports CPU-only workloads. To enable a broader class of applications that offload computations to hardware accelerators - GPU offload, NIC offload, FPGA offload, TPM communications - Gramine must be augmented with device-backed mmap support and generic ioctl support. In this paper, we describe the design and implementation of this newly added support, the corresponding changes to the manifest-file syntax and the requisite deep copy algorithm. We evaluate our implementation on Intel Media SDK workloads and discuss the encountered caveats and limitations. Finally, we outline a use case for the presented mmap/ioctl support beyond mere device communication, namely the mechanism to slice the application into the trusted enclave part (where the core application executes) and the untrusted shared-memory part (where insecure shared libraries execute).",
      "publication_date": "2022-03-02T07:28:53+00:00",
      "doi": "",
      "venue": "arXiv preprint",
      "citation_count": 0,
      "keywords": [
        "cs.CR"
      ],
      "arxiv_id": "2203.01813v1"
    },
    {
      "title": "FireFly: A High-Throughput Hardware Accelerator for Spiking Neural   Networks with Efficient DSP and Memory Optimization",
      "authors": [
        "Jindong Li",
        "Guobin Shen",
        "Dongcheng Zhao",
        "Qian Zhang",
        "Yi Zeng"
      ],
      "abstract": "Spiking neural networks (SNNs) have been widely used due to their strong biological interpretability and high energy efficiency. With the introduction of the backpropagation algorithm and surrogate gradient, the structure of spiking neural networks has become more complex, and the performance gap with artificial neural networks has gradually decreased. However, most SNN hardware implementations for field-programmable gate arrays (FPGAs) cannot meet arithmetic or memory efficiency requirements, which significantly restricts the development of SNNs. They do not delve into the arithmetic operations between the binary spikes and synaptic weights or assume unlimited on-chip RAM resources by using overly expensive devices on small tasks. To improve arithmetic efficiency, we analyze the neural dynamics of spiking neurons, generalize the SNN arithmetic operation to the multiplex-accumulate operation, and propose a high-performance implementation of such operation by utilizing the DSP48E2 hard block in Xilinx Ultrascale FPGAs. To improve memory efficiency, we design a memory system to enable efficient synaptic weights and membrane voltage memory access with reasonable on-chip RAM consumption. Combining the above two improvements, we propose an FPGA accelerator that can process spikes generated by the firing neuron on-the-fly (FireFly). FireFly is the first SNN accelerator that incorporates DSP optimization techniques into SNN synaptic operations. FireFly is implemented on several FPGA edge devices with limited resources but still guarantees a peak performance of 5.53TOP/s at 300MHz. As a lightweight accelerator, FireFly achieves the highest computational density efficiency compared with existing research using large FPGA devices.",
      "publication_date": "2023-01-05T04:28:07+00:00",
      "doi": "10.1109/TVLSI.2023.3279349",
      "venue": "arXiv preprint",
      "citation_count": 0,
      "keywords": [
        "cs.NE"
      ],
      "arxiv_id": "2301.01905v5"
    },
    {
      "title": "Lightweight Fault Detection Architecture for NTT on FPGA",
      "authors": [
        "Rourab Paul",
        "Paresh Baidya",
        "Krishnendu Guha"
      ],
      "abstract": "Post-Quantum Cryptographic (PQC) algorithms are mathematically secure and resistant to quantum attacks but can still leak sensitive information in hardware implementations due to natural faults or intentional fault injections. The intent fault injection in side-channel attacks reduces the reliability of crypto implementation in future generation network security procesors. In this regard, this research proposes a lightweight, efficient, recomputation-based fault detection module implemented on a Field Programmable Gate Array (FPGA) for Number Theoretic Transform (NTT). The NTT is primarily composed of memory units and the Cooley-Tukey Butterfly Unit (CT-BU), a critical and computationally intensive hardware component essential for polynomial multiplication. NTT and polynomial multiplication are fundamental building blocks in many PQC algorithms, including Kyber, NTRU, Ring-LWE, and others. In this paper, we present a fault detection method called : Recomputation with a Modular Offset (REMO) for the logic blocks of the CT-BU using Montgomery Reduction and another method called Memory Rule Checkers for the memory components used within the NTT. The proposed fault detection framework sets a new benchmark by achieving high efficiency with significant low implementation cost. It occupies only 16 slices and a single DSP block, with a power consumption of just 3mW in Artix-7 FPGA. The REMO-based detection mechanism achieves a fault coverage of 87.2% to 100%, adaptable across various word sizes, fault bit counts, and fault injection modes. Similarly, the Memory Rule Checkers demonstrate robust performance, achieving 50.7% to 100% fault detection depending on and the nature of injected faults.",
      "publication_date": "2025-08-05T04:23:50+00:00",
      "doi": "",
      "venue": "arXiv preprint",
      "citation_count": 0,
      "keywords": [
        "cs.CR"
      ],
      "arxiv_id": "2508.03062v1"
    },
    {
      "title": "Accelerating Implicit Finite Difference Schemes Using a Hardware   Optimized Tridiagonal Solver for FPGAs",
      "authors": [
        "Samuel Palmer"
      ],
      "abstract": "We present a design and implementation of the Thomas algorithm optimized for hardware acceleration on an FPGA, the Thomas Core. The hardware-based algorithm combined with the custom data flow and low level parallelism available in an FPGA reduces the overall complexity from 8N down to 5N serial arithmetic operations, and almost halves the overall latency by parallelizing the two costly divisions. Combining this with a data streaming interface, we reduce memory overheads to 2 N-length vectors per N-tridiagonal system to be solved. The Thomas Core allows for multiple independent tridiagonal systems to be continuously solved in parallel, providing an efficient and scalable accelerator for many numerical computations. Finally we present applications for derivatives pricing problems using implicit finite difference schemes on an FPGA accelerated system and we investigate the use and limitations of fixed-point arithmetic in our algorithm.",
      "publication_date": "2014-02-20T18:32:40+00:00",
      "doi": "",
      "venue": "arXiv preprint",
      "citation_count": 0,
      "keywords": [
        "q-fin.CP"
      ],
      "arxiv_id": "1402.5094v2"
    },
    {
      "title": "Accelerating Framework of Transformer by Hardware Design and Model   Compression Co-Optimization",
      "authors": [
        "Panjie Qi",
        "Edwin Hsing-Mean Sha",
        "Qingfeng Zhuge",
        "Hongwu Peng",
        "Shaoyi Huang",
        "Zhenglun Kong",
        "Yuhong Song",
        "Bingbing Li"
      ],
      "abstract": "State-of-the-art Transformer-based models, with gigantic parameters, are difficult to be accommodated on resource constrained embedded devices. Moreover, with the development of technology, more and more embedded devices are available to run a Transformer model. For a Transformer model with different constraints (tight or loose), it can be deployed onto devices with different computing power. However, in previous work, designers did not choose the best device among multiple devices. Instead, they just used an existing device to deploy model, which was not necessarily the best fit and may lead to underutilization of resources. To address the deployment challenge of Transformer and the problem to select the best device, we propose an algorithm & hardware closed-loop acceleration framework. Given a dataset, a model, latency constraint LC and accuracy constraint AC, our framework can provide a best device satisfying both constraints. In order to generate a compressed model with high sparsity ratio, we propose a novel pruning technique, hierarchical pruning (HP). We optimize the sparse matrix storage format for HP matrix to further reduce memory usage for FPGA implementation. We design a accelerator that takes advantage of HP to solve the problem of concurrent random access. Experiments on Transformer and TinyBert model show that our framework can find different devices for various LC and AC, covering from low-end devices to high-end devices. Our HP can achieve higher sparsity ratio and is more flexible than other sparsity pattern. Our framework can achieve 37x, 1.9x, 1.7x speedup compared to CPU, GPU and FPGA, respectively.",
      "publication_date": "2021-10-19T14:57:11+00:00",
      "doi": "",
      "venue": "arXiv preprint",
      "citation_count": 0,
      "keywords": [
        "cs.LG"
      ],
      "arxiv_id": "2110.10030v1"
    },
    {
      "title": "Efficient and Lightweight In-memory Computing Architecture for Hardware   Security",
      "authors": [
        "Hala Ajmi",
        "Fakhreddine Zayer",
        "Amira Hadj Fredj",
        "Belgacem Hamdi",
        "Baker Mohammad",
        "Naoufel Werghi",
        "Jorge Dias"
      ],
      "abstract": "The paper proposes in-memory computing (IMC) solution for the design and implementation of the Advanced Encryption Standard (AES) based cryptographic algorithm. This research aims at increasing the cyber security of autonomous driverless cars or robotic autonomous vehicles. The memristor (MR) designs are proposed in order to emulate the AES algorithm phases for efficient in-memory processing. The main features of this work are the following: a memristor 4bit state element is developed and used for implementing different arithmetic operations for AES hardware prototype; A pipeline AES design for massive parallelism and compatibility targeting MR integration; An FPGA implementation of AES-IMC based architecture with MR emulator. The AES-IMC outperforms existing architectures in both higher throughput, and energy efficiency. Compared with the conventional AES hardware, AES-IMC shows ~30% power enhancement with comparable throughput. As for state-of-the-art AES based NVM engines, AES-IMC has comparable power dissipation, and ~62% increased throughput. By enabling the cost-effective real-time deployment of the AES, the IMC architecture will prevent unintended accidents with unmanned devices caused by malicious attacks, including hijacking and unauthorized robot control.",
      "publication_date": "2022-05-24T08:39:44+00:00",
      "doi": "10.1016/j.jpdc.2024.104898",
      "venue": "arXiv preprint",
      "citation_count": 0,
      "keywords": [
        "cs.CR"
      ],
      "arxiv_id": "2205.11895v1"
    },
    {
      "title": "Distributed Hardware Accelerated Secure Joint Computation on the COPA   Framework",
      "authors": [
        "Rushi Patel",
        "Pouya Haghi",
        "Shweta Jain",
        "Andriy Kot",
        "Venkata Krishnan",
        "Mayank Varia",
        "Martin Herbordt"
      ],
      "abstract": "Performance of distributed data center applications can be improved through use of FPGA-based SmartNICs, which provide additional functionality and enable higher bandwidth communication. Until lately, however, the lack of a simple approach for customizing SmartNICs to application requirements has limited the potential benefits. Intel's Configurable Network Protocol Accelerator (COPA) provides a customizable FPGA framework that integrates both hardware and software development to improve computation and communication performance. In this first case study, we demonstrate the capabilities of the COPA framework with an application from cryptography -- secure Multi-Party Computation (MPC) -- that utilizes hardware accelerators connected directly to host memory and the COPA network. We find that using the COPA framework gives significant improvements to both computation and communication as compared to traditional implementations of MPC that use CPUs and NICs. A single MPC accelerator running on COPA enables more than 17Gbps of communication bandwidth while using only 1% of Stratix 10 resources. We show that utilizing the COPA framework enables multiple MPC accelerators running in parallel to fully saturate a 100Gbps link enabling higher performance compared to traditional NICs.",
      "publication_date": "2022-04-11T01:05:38+00:00",
      "doi": "",
      "venue": "arXiv preprint",
      "citation_count": 0,
      "keywords": [
        "cs.CR"
      ],
      "arxiv_id": "2204.04816v1"
    },
    {
      "title": "Medha: Microcoded Hardware Accelerator for computing on Encrypted Data",
      "authors": [
        "Ahmet Can Mert",
        "Aikata",
        "Sunmin Kwon",
        "Youngsam Shin",
        "Donghoon Yoo",
        "Yongwoo Lee",
        "Sujoy Sinha Roy"
      ],
      "abstract": "Homomorphic encryption (HE) enables computation on encrypted data, and hence it has a great potential in privacy-preserving outsourcing of computations to the cloud. Hardware acceleration of HE is crucial as software implementations are very slow. In this paper, we present design methodologies for building a programmable hardware accelerator for speeding up the cloud-side homomorphic evaluations on encrypted data. First, we propose a divide-and-conquer technique that enables homomorphic evaluations in a large polynomial ring $R_{Q,2N}$ to use a hardware accelerator that has been built for the smaller ring $R_{Q,N}$. The technique makes it possible to use a single hardware accelerator flexibly for supporting several HE parameter sets. Next, we present several architectural design methods that we use to realize the flexible and instruction-set accelerator architecture, which we call `Medha'. At every level of the implementation hierarchy, we explore possibilities for parallel processing. Starting from hardware-friendly parallel algorithms for the basic building blocks, we gradually build heavily parallel RNS polynomial arithmetic units. Next, many of these parallel units are interconnected elegantly so that their interconnections require the minimum number of nets, therefore making the overall architecture placement-friendly on the platform. For Medha, we take a memory-conservative design approach and get rid of any off-chip memory access during homomorphic evaluations. Finally, we implement Medha in a Xilinx Alveo U250 FPGA and measure timing performances of the microcoded homomorphic addition, multiplication, key-switching, and rescaling for the leveled HE scheme RNS-HEAAN at 200 MHz clock frequency. For two large parameter sets, Medha achieves accelerations by up to 68x and 78x times respectively compared to a highly optimized software implementation Microsoft SEAL running at 2.3 GHz.",
      "publication_date": "2022-10-11T14:21:54+00:00",
      "doi": "",
      "venue": "arXiv preprint",
      "citation_count": 0,
      "keywords": [
        "cs.CR"
      ],
      "arxiv_id": "2210.05476v2"
    },
    {
      "title": "Structured Weight Matrices-Based Hardware Accelerators in Deep Neural   Networks: FPGAs and ASICs",
      "authors": [
        "Caiwen Ding",
        "Ao Ren",
        "Geng Yuan",
        "Xiaolong Ma",
        "Jiayu Li",
        "Ning Liu",
        "Bo Yuan",
        "Yanzhi Wang"
      ],
      "abstract": "Both industry and academia have extensively investigated hardware accelerations. In this work, to address the increasing demands in computational capability and memory requirement, we propose structured weight matrices (SWM)-based compression techniques for both \\emph{field programmable gate array} (FPGA) and \\emph{application-specific integrated circuit} (ASIC) implementations. In algorithm part, SWM-based framework adopts block-circulant matrices to achieve a fine-grained tradeoff between accuracy and compression ratio. The SWM-based technique can reduce computational complexity from O($n^2$) to O($n\\log n$) and storage complexity from O($n^2$) to O($n$) for each layer and both training and inference phases. For FPGA implementations on deep convolutional neural networks (DCNNs), we achieve at least 152X and 72X improvement in performance and energy efficiency, respectively using the SWM-based framework, compared with the baseline of IBM TrueNorth processor under same accuracy constraints using the data set of MNIST, SVHN, and CIFAR-10. For FPGA implementations on long short term memory (LSTM) networks, the proposed SWM-based LSTM can achieve up to 21X enhancement in performance and 33.5X gains in energy efficiency compared with the baseline accelerator. For ASIC implementations, the SWM-based ASIC design exhibits impressive advantages in terms of power, throughput, and energy efficiency. Experimental results indicate that this method is greatly suitable for applying DNNs onto both FPGAs and mobile/IoT devices.",
      "publication_date": "2018-03-28T19:57:54+00:00",
      "doi": "",
      "venue": "arXiv preprint",
      "citation_count": 0,
      "keywords": [
        "cs.DC"
      ],
      "arxiv_id": "1804.11239v1"
    },
    {
      "title": "Ultra Memory-Efficient On-FPGA Training of Transformers via   Tensor-Compressed Optimization",
      "authors": [
        "Jiayi Tian",
        "Jinming Lu",
        "Hai Li",
        "Xiangwei Wang",
        "Cong Hao",
        "Ian Young",
        "Zheng Zhang"
      ],
      "abstract": "Transformer models have achieved state-of-the-art performance across a wide range of machine learning tasks. There is growing interest in training transformers on resource-constrained edge devices due to considerations such as privacy, domain adaptation, and on-device scientific machine learning. However, the significant computational and memory demands required for transformer training often exceed the capabilities of an edge device. Leveraging low-rank tensor compression, this paper presents the first on-FPGA accelerator for end-to-end transformer training. On the algorithm side, we present a bi-directional contraction flow for tensorized transformer training, significantly reducing the computational FLOPS and intra-layer memory costs compared to existing tensor operations. On the hardware side, we store all highly compressed model parameters and gradient information on chip, creating an on-chip-memory-only framework for each stage in training. This reduces off-chip communication and minimizes latency and energy costs. Additionally, we implement custom computing kernels for each training stage and employ intra-layer parallelism and pipe-lining to further enhance run-time and memory efficiency. Through experiments on transformer models within $36.7$ to $93.5$ MB using FP-32 data formats on the ATIS dataset, our tensorized FPGA accelerator could conduct single-batch end-to-end training on the AMD Alevo U50 FPGA, with a memory budget of less than $6$-MB BRAM and $22.5$-MB URAM. Compared to uncompressed training on the NVIDIA RTX 3090 GPU, our on-FPGA training achieves a memory reduction of $30\\times$ to $51\\times$. Our FPGA accelerator also achieves up to $3.6\\times$ less energy cost per epoch compared with tensor Transformer training on an NVIDIA RTX 3090 GPU.",
      "publication_date": "2025-01-11T23:29:51+00:00",
      "doi": "",
      "venue": "arXiv preprint",
      "citation_count": 0,
      "keywords": [
        "cs.LG"
      ],
      "arxiv_id": "2501.06663v2"
    },
    {
      "title": "Femto-Containers: Lightweight Virtualization and Fault Isolation For   Small Software Functions on Low-Power IoT Microcontrollers",
      "authors": [
        "Koen Zandberg",
        "Emmanuel Baccelli",
        "Shenghao Yuan",
        "Frédéric Besson",
        "Jean-Pierre Talpin"
      ],
      "abstract": "Low-power operating system runtimes used on IoT microcontrollers typically provide rudimentary APIs, basic connectivity and, sometimes, a (secure) firmware update mechanism. In contrast, on less constrained hardware, networked software has entered the age of serverless, microservices and agility. With a view to bridge this gap, in the paper we design Femto-Containers, a new middleware runtime which can be embedded on heterogeneous low-power IoT devices. Femto-Containers enable the secure deployment, execution and isolation of small virtual software functions on low-power IoT devices, over the network. We implement Femto-Containers, and provide integration in RIOT, a popular open source IoT operating system. We then evaluate the performance of our implementation, which was formally verified for fault-isolation, guaranteeing that RIOT is shielded from logic loaded and executed in a Femto-Container. Our experiments on various popular microcontroller architectures (Arm Cortex-M, ESP32 and RISC-V) show that Femto-Containers offer an attractive trade-off in terms of memory footprint overhead, energy consumption, and security",
      "publication_date": "2022-10-07T10:03:55+00:00",
      "doi": "",
      "venue": "arXiv preprint",
      "citation_count": 0,
      "keywords": [
        "cs.OS"
      ],
      "arxiv_id": "2210.03432v1"
    },
    {
      "title": "Lightweight Software Kernels and Hardware Extensions for Efficient   Sparse Deep Neural Networks on Microcontrollers",
      "authors": [
        "Francesco Daghero",
        "Daniele Jahier Pagliari",
        "Francesco Conti",
        "Luca Benini",
        "Massimo Poncino",
        "Alessio Burrello"
      ],
      "abstract": "The acceleration of pruned Deep Neural Networks (DNNs) on edge devices such as Microcontrollers (MCUs) is a challenging task, given the tight area- and power-constraints of these devices. In this work, we propose a three-fold contribution to address this problem. First, we design a set of optimized software kernels for N:M pruned layers, targeting ultra-low-power, multicore RISC-V MCUs, which are up to 2.1x and 3.4x faster than their dense counterparts at 1:8 and 1:16 sparsity, respectively. Then, we implement a lightweight Instruction-Set Architecture (ISA) extension to accelerate the indirect load and non-zero indices decompression operations required by our kernels, obtaining up to 1.9x extra speedup, at the cost of a 5% area overhead. Lastly, we extend an open-source DNN compiler to utilize our sparse kernels for complete networks, showing speedups of 3.21x and 1.81x on a ResNet18 and a Vision Transformer (ViT), with less than 1.5% accuracy drop compared to a dense baseline.",
      "publication_date": "2025-03-08T11:59:12+00:00",
      "doi": "",
      "venue": "arXiv preprint",
      "citation_count": 0,
      "keywords": [
        "cs.LG"
      ],
      "arxiv_id": "2503.06183v2"
    },
    {
      "title": "HW/SW Implementation of MiRitH on Embedded Platforms",
      "authors": [
        "Maximilian Schöffel",
        "Hiandra Tomasi",
        "Norbert Wehn"
      ],
      "abstract": "Multi-Party Computation in the Head (MPCitH) algorithms are appealing candidates in the additional US NIST standardization rounds for Post-Quantum Cryptography (PQC) with respect to key sizes and mathematical hardness assumptions. However, their complexity presents a significant challenge for platforms with limited computational capabilities. To address this issue, we present, to the best of our knowledge, the first design space exploration of MiRitH, a promising MPCitH algorithm, for embedded devices. We develop a library of mixed HW/SW blocks on the Xilinx ZYNQ 7000, and, based on this library, we explore optimal solutions under runtime or FPGA resource constraints for a given public key infrastructure. Our results show that MiRitH is a viable algorithm for embedded devices in terms of runtime and FPGA resource requirements.",
      "publication_date": "2024-11-19T08:30:08+00:00",
      "doi": "",
      "venue": "arXiv preprint",
      "citation_count": 0,
      "keywords": [
        "cs.CR"
      ],
      "arxiv_id": "2411.12328v1"
    },
    {
      "title": "PQC-HA: A Framework for Prototyping and In-Hardware Evaluation of   Post-Quantum Cryptography Hardware Accelerators",
      "authors": [
        "Richard Sattel",
        "Christoph Spang",
        "Carsten Heinz",
        "Andreas Koch"
      ],
      "abstract": "In the third round of the NIST Post-Quantum Cryptography standardization project, the focus is on optimizing software and hardware implementations of candidate schemes. The winning schemes are CRYSTALS Kyber and CRYSTALS Dilithium, which serve as a Key Encapsulation Mechanism (KEM) and Digital Signature Algorithm (DSA), respectively. This study utilizes the TaPaSCo open-source framework to create hardware building blocks for both schemes using High-level Synthesis (HLS) from minimally modified ANSI C software reference implementations across all security levels. Additionally, a generic TaPaSCo host runtime application is developed in Rust to verify their functionality through the standard NIST interface, utilizing the corresponding Known Answer Test mechanism on actual hardware. Building on this foundation, the communication overhead for TaPaSCo hardware accelerators on PCIe-connected FPGA devices is evaluated and compared with previous work and optimized AVX2 software reference implementations. The results demonstrate the feasibility of verifying and evaluating the performance of Post-Quantum Cryptography accelerators on real hardware using TaPaSCo. Furthermore, the off-chip accelerator communication overhead of the NIST standard interface is measured, which, on its own, outweighs the execution wall clock time of the optimized software reference implementation of Kyber at Security Level 1.",
      "publication_date": "2023-08-12T17:35:39+00:00",
      "doi": "",
      "venue": "arXiv preprint",
      "citation_count": 0,
      "keywords": [
        "cs.CR"
      ],
      "arxiv_id": "2308.06621v1"
    },
    {
      "title": "Tile optimization for area in FPGA based hardware acceleration of   peptide identification",
      "authors": [
        "S. M. Vidanagamachchi",
        "S. D. Dewasurendra",
        "R. G. Ragel",
        "M. Niranjan"
      ],
      "abstract": "Advances in life sciences over the last few decades have lead to the generation of a huge amount of biological data. Computing research has become a vital part in driving biological discovery where analysis and categorization of biological data are involved. String matching algorithms can be applied for protein/gene sequence matching and with the phenomenal increase in the size of string databases to be analyzed, software implementations of these algorithms seems to have hit a hard limit and hardware acceleration is increasingly being sought. Several hardware platforms such as Field Programmable Gate Arrays (FPGA), Graphics Processing Units (GPU) and Chip Multi Processors (CMP) are being explored as hardware platforms. In this paper, we give a comprehensive overview of the literature on hardware acceleration of string matching algorithms, we take an FPGA hardware exploration and expedite the design time by a design automation technique. Further, our design automation is also optimized for better hardware utilization through optimizing the number of peptides that can be represented in an FPGA tile. The results indicate significant improvements in design time and hardware utilization which are reported in this paper.",
      "publication_date": "2014-03-28T08:04:43+00:00",
      "doi": "10.1109/ICIINFS.2011.6038056",
      "venue": "arXiv preprint",
      "citation_count": 0,
      "keywords": [
        "cs.CE"
      ],
      "arxiv_id": "1403.7296v1"
    },
    {
      "title": "FireFly v2: Advancing Hardware Support for High-Performance Spiking   Neural Network with a Spatiotemporal FPGA Accelerator",
      "authors": [
        "Jindong Li",
        "Guobin Shen",
        "Dongcheng Zhao",
        "Qian Zhang",
        "Yi Zeng"
      ],
      "abstract": "Spiking Neural Networks (SNNs) are expected to be a promising alternative to Artificial Neural Networks (ANNs) due to their strong biological interpretability and high energy efficiency. Specialized SNN hardware offers clear advantages over general-purpose devices in terms of power and performance. However, there's still room to advance hardware support for state-of-the-art (SOTA) SNN algorithms and improve computation and memory efficiency. As a further step in supporting high-performance SNNs on specialized hardware, we introduce FireFly v2, an FPGA SNN accelerator that can address the issue of non-spike operation in current SOTA SNN algorithms, which presents an obstacle in the end-to-end deployment onto existing SNN hardware. To more effectively align with the SNN characteristics, we design a spatiotemporal dataflow that allows four dimensions of parallelism and eliminates the need for membrane potential storage, enabling on-the-fly spike processing and spike generation. To further improve hardware acceleration performance, we develop a high-performance spike computing engine as a backend based on a systolic array operating at 500-600MHz. To the best of our knowledge, FireFly v2 achieves the highest clock frequency among all FPGA-based implementations. Furthermore, it stands as the first SNN accelerator capable of supporting non-spike operations, which are commonly used in advanced SNN algorithms. FireFly v2 has doubled the throughput and DSP efficiency when compared to our previous version of FireFly and it exhibits 1.33 times the DSP efficiency and 1.42 times the power efficiency compared to the current most advanced FPGA accelerators.",
      "publication_date": "2023-09-28T04:17:02+00:00",
      "doi": "",
      "venue": "arXiv preprint",
      "citation_count": 0,
      "keywords": [
        "cs.NE"
      ],
      "arxiv_id": "2309.16158v1"
    },
    {
      "title": "FPGA-based Neural Network Accelerator for Millimeter-Wave   Radio-over-Fiber Systems",
      "authors": [
        "Jeonghun Lee",
        "Jiayuan He",
        "Ke Wang"
      ],
      "abstract": "With the rapidly-developing high-speed wireless communications, the 60 GHz millimeter-wave frequency range and radio-over-fiber systems have been investigated as a promising solution to deliver mm-wave signals. Neural networks have been studied to improve the mm-wave RoF system performances at the receiver side by suppressing linear and nonlinear impairments. However, previous neural network studies in mm-wave RoF systems focus on the off-line implementation with high-end GPUs , which is not practical for low power-consumption, low-cost and limited computation platform applications. To solve this issue, we investigate neural network hardware accelerator implementations using the field programmable gate array (FPGA), taking advantage of the low power consumption, parallel computation capability, and reconfigurablity features of FPGA. Convolutional neural network (CNN) and binary convolutional neural network (BCNN) hardware accelerators are demonstrated. In addition, to satisfy the low-latency requirement in mm-wave RoF systems and to enable the use of low-cost compact FPGA devices, a novel inner parallel optimization method is proposed. Compared with the embedded processor (ARM Cortex A9) execution latency, the CNN/BCNN FPGA-based hardware accelerator reduces their latency by over 92%. Compared with non-optimized FPGA implementations, the proposed optimization method reduces the processing latency by over 44% for CNN and BCNN. Compared with the GPU implementation, the latency of CNN implementation with the proposed optimization method is reduced by 85.49%, while the power consumption is reduced by 86.91%. Although the latency of BCNN implementation with the proposed optimization method is larger compared with the GPU implementation, the power consumption is reduced by 86.14%. The FPGA-based neural network hardware accelerators provide a promising solution for mm-wave RoF systems.",
      "publication_date": "2020-02-19T14:21:19+00:00",
      "doi": "10.1364/OE.391050",
      "venue": "arXiv preprint",
      "citation_count": 0,
      "keywords": [
        "eess.SP"
      ],
      "arxiv_id": "2002.08205v1"
    },
    {
      "title": "Towards Budget-Driven Hardware Optimization for Deep Convolutional   Neural Networks using Stochastic Computing",
      "authors": [
        "Zhe Li",
        "Ji Li",
        "Ao Ren",
        "Caiwen Ding",
        "Jeffrey Draper",
        "Qinru Qiu",
        "Bo Yuan",
        "Yanzhi Wang"
      ],
      "abstract": "Recently, Deep Convolutional Neural Network (DCNN) has achieved tremendous success in many machine learning applications. Nevertheless, the deep structure has brought significant increases in computation complexity. Largescale deep learning systems mainly operate in high-performance server clusters, thus restricting the application extensions to personal or mobile devices. Previous works on GPU and/or FPGA acceleration for DCNNs show increasing speedup, but ignore other constraints, such as area, power, and energy. Stochastic Computing (SC), as a unique data representation and processing technique, has the potential to enable the design of fully parallel and scalable hardware implementations of large-scale deep learning systems. This paper proposed an automatic design allocation algorithm driven by budget requirement considering overall accuracy performance. This systematic method enables the automatic design of a DCNN where all design parameters are jointly optimized. Experimental results demonstrate that proposed algorithm can achieve a joint optimization of all design parameters given the comprehensive budget of a DCNN.",
      "publication_date": "2018-05-10T19:21:39+00:00",
      "doi": "",
      "venue": "arXiv preprint",
      "citation_count": 0,
      "keywords": [
        "cs.NE"
      ],
      "arxiv_id": "1805.04142v1"
    },
    {
      "title": "Optimization of FPGA-based CNN Accelerators Using Metaheuristics",
      "authors": [
        "Sadiq M. Sait",
        "Aiman El-Maleh",
        "Mohammad Altakrouri",
        "Ahmad Shawahna"
      ],
      "abstract": "In recent years, convolutional neural networks (CNNs) have demonstrated their ability to solve problems in many fields and with accuracy that was not possible before. However, this comes with extensive computational requirements, which made general CPUs unable to deliver the desired real-time performance. At the same time, FPGAs have seen a surge in interest for accelerating CNN inference. This is due to their ability to create custom designs with different levels of parallelism. Furthermore, FPGAs provide better performance per watt compared to GPUs. The current trend in FPGA-based CNN accelerators is to implement multiple convolutional layer processors (CLPs), each of which is tailored for a subset of layers. However, the growing complexity of CNN architectures makes optimizing the resources available on the target FPGA device to deliver optimal performance more challenging. In this paper, we present a CNN accelerator and an accompanying automated design methodology that employs metaheuristics for partitioning available FPGA resources to design a Multi-CLP accelerator. Specifically, the proposed design tool adopts simulated annealing (SA) and tabu search (TS) algorithms to find the number of CLPs required and their respective configurations to achieve optimal performance on a given target FPGA device. Here, the focus is on the key specifications and hardware resources, including digital signal processors, block RAMs, and off-chip memory bandwidth. Experimental results and comparisons using four well-known benchmark CNNs are presented demonstrating that the proposed acceleration framework is both encouraging and promising. The SA-/TS-based Multi-CLP achieves 1.31x - 2.37x higher throughput than the state-of-the-art Single-/Multi-CLP approaches in accelerating AlexNet, SqueezeNet 1.1, VGGNet, and GoogLeNet architectures on the Xilinx VC707 and VC709 FPGA boards.",
      "publication_date": "2022-09-22T18:57:49+00:00",
      "doi": "10.1007/s11227-022-04787-8",
      "venue": "arXiv preprint",
      "citation_count": 0,
      "keywords": [
        "cs.NE"
      ],
      "arxiv_id": "2209.11272v1"
    },
    {
      "title": "Toward a Lightweight, Scalable, and Parallel Secure Encryption Engine",
      "authors": [
        "Rasha Karakchi",
        "Rye Stahle-Smith",
        "Nishant Chinnasami",
        "Tiffany Yu"
      ],
      "abstract": "The exponential growth of Internet of Things (IoT) applications has intensified the demand for efficient, high-throughput, and energy-efficient data processing at the edge. Conventional CPU-centric encryption methods suffer from performance bottlenecks and excessive data movement, especially in latency-sensitive and resource-constrained environments. In this paper, we present SPiME, a lightweight, scalable, and FPGA-compatible Secure Processor-in-Memory Encryption architecture that integrates the Advanced Encryption Standard (AES-128) directly into a Processing-in-Memory (PiM) framework. SPiME is designed as a modular array of parallel PiM units, each combining an AES core with a minimal control unit to enable distributed in-place encryption with minimal overhead. The architecture is fully implemented in Verilog and tested on multiple AMD UltraScale and UltraScale+ FPGAs. Evaluation results show that SPiME can scale beyond 4,000 parallel units while maintaining less than 5\\% utilization of key FPGA resources on high-end devices. It delivers over 25~Gbps in sustained encryption throughput with predictable, low-latency performance. The design's portability, configurability, and resource efficiency make it a compelling solution for secure edge computing, embedded cryptographic systems, and customizable hardware accelerators.",
      "publication_date": "2025-06-18T02:25:04+00:00",
      "doi": "",
      "venue": "arXiv preprint",
      "citation_count": 0,
      "keywords": [
        "cs.CR"
      ],
      "arxiv_id": "2506.15070v1"
    },
    {
      "title": "FPGA-Based Hardware Accelerator of Homomorphic Encryption for Efficient   Federated Learning",
      "authors": [
        "Zhaoxiong Yang",
        "Shuihai Hu",
        "Kai Chen"
      ],
      "abstract": "With the increasing awareness of privacy protection and data fragmentation problem, federated learning has been emerging as a new paradigm of machine learning. Federated learning tends to utilize various privacy preserving mechanisms to protect the transferred intermediate data, among which homomorphic encryption strikes a balance between security and ease of utilization. However, the complicated operations and large operands impose significant overhead on federated learning. Maintaining accuracy and security more efficiently has been a key problem of federated learning. In this work, we investigate a hardware solution, and design an FPGA-based homomorphic encryption framework, aiming to accelerate the training phase in federated learning. The root complexity lies in searching for a compact architecture for the core operation of homomorphic encryption, to suit the requirement of federated learning about high encryption throughput and flexibility of configuration. Our framework implements the representative Paillier homomorphic cryptosystem with high level synthesis for flexibility and portability, with careful optimization on the modular multiplication operation in terms of processing clock cycle, resource usage and clock frequency. Our accelerator achieves a near-optimal execution clock cycle, with a better DSP-efficiency than existing designs, and reduces the encryption time by up to 71% during training process of various federated learning models.",
      "publication_date": "2020-07-21T01:59:58+00:00",
      "doi": "",
      "venue": "arXiv preprint",
      "citation_count": 0,
      "keywords": [
        "cs.CR"
      ],
      "arxiv_id": "2007.10560v1"
    },
    {
      "title": "An Efficient SDN Architecture for Smart Home Security Accelerated by   FPGA",
      "authors": [
        "Holden Gordon",
        "Conrad Park",
        "Bhagyashri Tushir",
        "Yuhong Liu",
        "Behnam Dezfouli"
      ],
      "abstract": "With the rise in Internet of Things (IoT) devices, home network management and security are becoming complex. There is an urgent requirement to make smart home network management efficient. This work proposes an SDN-based architecture to secure smart home networks through K-Nearest Neighbor (KNN) based device classifications and malicious traffic detection. The efficiency is further enhanced by offloading the computation-intensive KNN model to Field Programmable Gate Arrays (FPGA), which offers parallel processing power of GPU platforms at lower costs and higher efficiencies, and can be used to accelerate time-sensitive tasks. The proposed parallelization and implementation of KNN on FPGA are achieved by using the Vivado Design Suite from Xilinx and High-Level Synthesis (HLS). When optimized with 10-fold cross-validation, the proposed solution for KNN consistently exhibits the best performances on FPGA when compared with four alternative KNN instances (i.e., 78% faster than the parallel bubble sort-based implementation and 99\\% faster than the other three sorting algorithms). Moreover, with 36,225 training samples, the proposed KNN solution classifies a test query with 95% accuracy in approximately 4 milliseconds on FPGA compared to 57 seconds on a CPU platform.",
      "publication_date": "2021-06-21T19:59:14+00:00",
      "doi": "",
      "venue": "arXiv preprint",
      "citation_count": 0,
      "keywords": [
        "cs.CR"
      ],
      "arxiv_id": "2106.11390v2"
    },
    {
      "title": "Secure Blockchain Platform for Industrial IoT with Trusted Computing   Hardware",
      "authors": [
        "Qing Yang",
        "Hao Wang",
        "Xiaoxiao Wu",
        "Taotao Wang",
        "Shengli Zhang",
        "Naijin Liu"
      ],
      "abstract": "As a disruptive technology that originates from cryptocurrency, blockchain provides a trusted platform to facilitate industrial IoT (IIoT) applications. However, implementing a blockchain platform in IIoT scenarios confronts various security challenges due to the rigorous deployment condition. To this end, we present a novel design of secure blockchain based on trusted computing hardware for IIoT applications. Specifically, we employ the trusted execution environment (TEE) module and a customized security chip to safeguard the blockchain against different attacking vectors. Furthermore, we implement the proposed secure IIoT blockchain on the ARM-based embedded device and build a small-scale IIoT network to evaluate its performance. Our experimental results show that the secure blockchain platform achieves a high throughput (150TPS) with low transaction confirmation delay (below 66ms), demonstrating its feasibility in practical IIoT scenarios. Finally, we outline the open challenges and future research directions.",
      "publication_date": "2021-10-28T14:37:01+00:00",
      "doi": "10.1109/IOTM.001.2100043",
      "venue": "arXiv preprint",
      "citation_count": 0,
      "keywords": [
        "cs.CR"
      ],
      "arxiv_id": "2110.15161v1"
    },
    {
      "title": "Algorithm-hardware Co-design for Deformable Convolution",
      "authors": [
        "Qijing Huang",
        "Dequan Wang",
        "Yizhao Gao",
        "Yaohui Cai",
        "Zhen Dong",
        "Bichen Wu",
        "Kurt Keutzer",
        "John Wawrzynek"
      ],
      "abstract": "FPGAs provide a flexible and efficient platform to accelerate rapidly-changing algorithms for computer vision. The majority of existing work focuses on accelerating image classification, while other fundamental vision problems, including object detection and instance segmentation, have not been adequately addressed. Compared with image classification, detection problems are more sensitive to the spatial variance of objects, and therefore, require specialized convolutions to aggregate spatial information. To address this, recent work proposes dynamic deformable convolution to augment regular convolutions. Regular convolutions process a fixed grid of pixels across all the spatial locations in an image, while dynamic deformable convolutions may access arbitrary pixels in the image and the access pattern is input-dependent and varies per spatial location. These properties lead to inefficient memory accesses of inputs with existing hardware. In this work, we first investigate the overhead of the deformable convolution on embedded FPGA SoCs, and then show the accuracy-latency tradeoffs for a set of algorithm modifications including full versus depthwise, fixed-shape, and limited-range. These modifications benefit the energy efficiency for embedded devices in general as they reduce the compute complexity. We then build an efficient object detection network with modified deformable convolutions and quantize the network using state-of-the-art quantization methods. We implement a unified hardware engine on FPGA to support all the operations in the network. Preliminary experiments show that little accuracy is compromised and speedup can be achieved with our co-design optimization for the deformable convolution.",
      "publication_date": "2020-02-19T01:08:11+00:00",
      "doi": "",
      "venue": "arXiv preprint",
      "citation_count": 0,
      "keywords": [
        "eess.IV"
      ],
      "arxiv_id": "2002.08357v1"
    },
    {
      "title": "FLAIRS: FPGA-Accelerated Inference-Resistant & Secure Federated Learning",
      "authors": [
        "Huimin Li",
        "Phillip Rieger",
        "Shaza Zeitouni",
        "Stjepan Picek",
        "Ahmad-Reza Sadeghi"
      ],
      "abstract": "Federated Learning (FL) has become very popular since it enables clients to train a joint model collaboratively without sharing their private data. However, FL has been shown to be susceptible to backdoor and inference attacks. While in the former, the adversary injects manipulated updates into the aggregation process; the latter leverages clients' local models to deduce their private data. Contemporary solutions to address the security concerns of FL are either impractical for real-world deployment due to high-performance overheads or are tailored towards addressing specific threats, for instance, privacy-preserving aggregation or backdoor defenses. Given these limitations, our research delves into the advantages of harnessing the FPGA-based computing paradigm to overcome performance bottlenecks of software-only solutions while mitigating backdoor and inference attacks. We utilize FPGA-based enclaves to address inference attacks during the aggregation process of FL. We adopt an advanced backdoor-aware aggregation algorithm on the FPGA to counter backdoor attacks. We implemented and evaluated our method on Xilinx VMK-180, yielding a significant speed-up of around 300 times on the IoT-Traffic dataset and more than 506 times on the CIFAR-10 dataset.",
      "publication_date": "2023-08-01T13:47:27+00:00",
      "doi": "",
      "venue": "arXiv preprint",
      "citation_count": 0,
      "keywords": [
        "cs.CR"
      ],
      "arxiv_id": "2308.00553v1"
    },
    {
      "title": "Performance Analysis and Industry Deployment of Post-Quantum   Cryptography Algorithms",
      "authors": [
        "Elif Dicle Demir",
        "Buse Bilgin",
        "Mehmet Cengiz Onbasli"
      ],
      "abstract": "As quantum computing advances, modern cryptographic standards face an existential threat, necessitating a transition to post-quantum cryptography (PQC). The National Institute of Standards and Technology (NIST) has selected CRYSTALS-Kyber and CRYSTALS-Dilithium as standardized PQC algorithms for secure key exchange and digital signatures, respectively. This study conducts a comprehensive performance analysis of these algorithms by benchmarking execution times across cryptographic operations such as key generation, encapsulation, decapsulation, signing, and verification. Additionally, the impact of AVX2 optimizations is evaluated to assess hardware acceleration benefits. Our findings demonstrate that Kyber and Dilithium achieve efficient execution times, outperforming classical cryptographic schemes such as RSA and ECDSA at equivalent security levels. Beyond technical performance, the real-world deployment of PQC introduces challenges in telecommunications networks, where large-scale infrastructure upgrades, interoperability with legacy systems, and regulatory constraints must be addressed. This paper examines the feasibility of PQC adoption in telecom environments, highlighting key transition challenges, security risks, and implementation strategies. Through industry case studies, we illustrate how telecom operators are integrating PQC into 5G authentication, subscriber identity protection, and secure communications. Our analysis provides insights into the computational trade-offs, deployment considerations, and standardization efforts shaping the future of quantum-safe cryptographic infrastructure.",
      "publication_date": "2025-03-17T09:06:03+00:00",
      "doi": "",
      "venue": "arXiv preprint",
      "citation_count": 0,
      "keywords": [
        "cs.CR"
      ],
      "arxiv_id": "2503.12952v2"
    },
    {
      "title": "CURE: A Security Architecture with CUstomizable and Resilient Enclaves",
      "authors": [
        "Raad Bahmani",
        "Ferdinand Brasser",
        "Ghada Dessouky",
        "Patrick Jauernig",
        "Matthias Klimmek",
        "Ahmad-Reza Sadeghi",
        "Emmanuel Stapf"
      ],
      "abstract": "Security architectures providing Trusted Execution Environments (TEEs) have been an appealing research subject for a wide range of computer systems, from low-end embedded devices to powerful cloud servers. The goal of these architectures is to protect sensitive services in isolated execution contexts, called enclaves. Unfortunately, existing TEE solutions suffer from significant design shortcomings. First, they follow a one-size-fits-all approach offering only a single enclave type, however, different services need flexible enclaves that can adjust to their demands. Second, they cannot efficiently support emerging applications (e.g., Machine Learning as a Service), which require secure channels to peripherals (e.g., accelerators), or the computational power of multiple cores. Third, their protection against cache side-channel attacks is either an afterthought or impractical, i.e., no fine-grained mapping between cache resources and individual enclaves is provided.   In this work, we propose CURE, the first security architecture, which tackles these design challenges by providing different types of enclaves: (i) sub-space enclaves provide vertical isolation at all execution privilege levels, (ii) user-space enclaves provide isolated execution to unprivileged applications, and (iii) self-contained enclaves allow isolated execution environments that span multiple privilege levels. Moreover, CURE enables the exclusive assignment of system resources, e.g., peripherals, CPU cores, or cache resources to single enclaves. CURE requires minimal hardware changes while significantly improving the state of the art of hardware-assisted security architectures. We implemented CURE on a RISC-V-based SoC and thoroughly evaluated our prototype in terms of hardware and performance overhead. CURE imposes a geometric mean performance overhead of 15.33% on standard benchmarks.",
      "publication_date": "2020-10-29T18:11:14+00:00",
      "doi": "",
      "venue": "arXiv preprint",
      "citation_count": 0,
      "keywords": [
        "cs.CR"
      ],
      "arxiv_id": "2010.15866v1"
    },
    {
      "title": "BASALISC: Programmable Hardware Accelerator for BGV Fully Homomorphic   Encryption",
      "authors": [
        "Robin Geelen",
        "Michiel Van Beirendonck",
        "Hilder V. L. Pereira",
        "Brian Huffman",
        "Tynan McAuley",
        "Ben Selfridge",
        "Daniel Wagner",
        "Georgios Dimou",
        "Ingrid Verbauwhede",
        "Frederik Vercauteren",
        "David W. Archer"
      ],
      "abstract": "Fully Homomorphic Encryption (FHE) allows for secure computation on encrypted data. Unfortunately, huge memory size, computational cost and bandwidth requirements limit its practicality. We present BASALISC, an architecture family of hardware accelerators that aims to substantially accelerate FHE computations in the cloud. BASALISC is the first to implement the BGV scheme with fully-packed bootstrapping -- the noise removal capability necessary for arbitrary-depth computation. It supports a customized version of bootstrapping that can be instantiated with hardware multipliers optimized for area and power.   BASALISC is a three-abstraction-layer RISC architecture, designed for a 1 GHz ASIC implementation and underway toward 150mm2 die tape-out in a 12nm GF process. BASALISC's four-layer memory hierarchy includes a two-dimensional conflict-free inner memory layer that enables 32 Tb/s radix-256 NTT computations without pipeline stalls. Its conflict-resolution permutation hardware is generalized and re-used to compute BGV automorphisms without throughput penalty. BASALISC also has a custom multiply-accumulate unit to accelerate BGV key switching.   The BASALISC toolchain comprises a custom compiler and a joint performance and correctness simulator. To evaluate BASALISC, we study its physical realizability, emulate and formally verify its core functional units, and we study its performance on a set of benchmarks. Simulation results show a speedup of more than 5,000 times over HElib -- a popular software FHE library.",
      "publication_date": "2022-05-27T14:37:38+00:00",
      "doi": "",
      "venue": "arXiv preprint",
      "citation_count": 0,
      "keywords": [
        "cs.CR"
      ],
      "arxiv_id": "2205.14017v3"
    },
    {
      "title": "CoMeFa: Compute-in-Memory Blocks for FPGAs",
      "authors": [
        "Aman Arora",
        "Tanmay Anand",
        "Aatman Borda",
        "Rishabh Sehgal",
        "Bagus Hanindhito",
        "Jaydeep Kulkarni",
        "Lizy K. John"
      ],
      "abstract": "Block RAMs (BRAMs) are the storage houses of FPGAs, providing extensive on-chip memory bandwidth to the compute units implemented using Logic Blocks (LBs) and Digital Signal Processing (DSP) slices. We propose modifying BRAMs to convert them to CoMeFa (Compute-In-Memory Blocks for FPGAs) RAMs. These RAMs provide highly-parallel compute-in-memory by combining computation and storage capabilities in one block. CoMeFa RAMs utilize the true dual port nature of FPGA BRAMs and contain multiple programmable single-bit bit-serial processing elements. CoMeFa RAMs can be used to compute in any precision, which is extremely important for evolving applications like Deep Learning. Adding CoMeFa RAMs to FPGAs significantly increases their compute density. We explore and propose two architectures of these RAMs: CoMeFa-D (optimized for delay) and CoMeFa-A (optimized for area). Compared to existing proposals, CoMeFa RAMs do not require changing the underlying SRAM technology like simultaneously activating multiple rows on the same port, and are practical to implement. CoMeFa RAMs are versatile blocks that find applications in numerous diverse parallel applications like Deep Learning, signal processing, databases, etc. By augmenting an Intel Arria-10-like FPGA with CoMeFa-D (CoMeFa-A) RAMs at the cost of 3.8% (1.2%) area, and with algorithmic improvements and efficient mapping, we observe a geomean speedup of 2.5x (1.8x), across several representative benchmarks. Replacing all or some BRAMs with CoMeFa RAMs in FPGAs can make them better accelerators of modern compute-intensive workloads.",
      "publication_date": "2022-03-23T16:31:46+00:00",
      "doi": "",
      "venue": "arXiv preprint",
      "citation_count": 0,
      "keywords": [
        "cs.AR"
      ],
      "arxiv_id": "2203.12521v1"
    },
    {
      "title": "Near Threshold Computation of Partitioned Ring Learning With Error   (RLWE) Post Quantum Cryptography on Reconfigurable Architecture",
      "authors": [
        "Paresh Baidya",
        "Swagata Mondal",
        "Rourab Paul"
      ],
      "abstract": "Ring Learning With Error (RLWE) algorithm is used in Post Quantum Cryptography (PQC) and Homomorphic Encryption (HE) algorithm. The existing classical crypto algorithms may be broken in quantum computers. The adversaries can store all encrypted data. While the quantum computer will be available, these encrypted data can be exposed by the quantum computer. Therefore, the PQC algorithms are an essential solution in recent applications. On the other hand, the HE allows operations on encrypted data which is appropriate for getting services from third parties without revealing confidential plain-texts. The FPGA based PQC and HE hardware accelerators like RLWE is much cost-effective than processor based platform and Application Specific Integrated Circuit (ASIC). FPGA based hardware accelerators still consume more power compare to ASIC based design. Near Threshold Computation (NTC) may be a convenient solution for FPGA based RLWE implementation. In this paper, we have implemented RLWE hardware accelerator which has 14 subcomponents. This paper creates clusters based on the critical path of all 14 subcomponents. Each cluster is implemented in an FPGA partition which has the same biasing voltage $V_{ccint}$. The clusters that have higher critical paths use higher Vccint to avoid timing failure. The clusters have lower critical paths use lower biasing voltage Vccint. This voltage scaled, partitioned RLWE can save ~6% and ~11% power in Vivado and VTR platform respectively. The resource usage and throughput of the implemented RLWE hardware accelerator is comparatively better than existing literature.",
      "publication_date": "2022-08-17T06:08:54+00:00",
      "doi": "",
      "venue": "arXiv preprint",
      "citation_count": 0,
      "keywords": [
        "cs.CR"
      ],
      "arxiv_id": "2208.08093v2"
    },
    {
      "title": "Code-based Cryptography in IoT: A HW/SW Co-Design of HQC",
      "authors": [
        "Maximilian Schöffel",
        "Johannes Feldmann",
        "Norbert Wehn"
      ],
      "abstract": "Recent advances in quantum computing pose a serious threat on the security of widely used public-key cryptosystems. Thus, new post-quantum cryptographic algorithms have been proposed as part of the associated US NIST process to enable secure, encrypted communication in the age of quantum computing. Many hardware accelerators for structured lattice-based algorithms have already been published to meet the strict power, area and latency requirements of low-power IoT edge devices. However, the security of these algorithms is still uncertain. Currently, many new attacks against the lattice structure are investigated to judge on their security. In contrast, code-based algorithms, which rely on deeply explored security metrics and are appealing candidates in the NIST process, have not yet been investigated to the same depth in the context of IoT due to the computational complexity and memory footprint of state-of-the-art software implementations.   In this paper, we present to the best of our knowledge the first HW/SW co-design based implementation of the code-based Hamming Quasi Cyclic Key-Encapsulation Mechanism. We profile and evaluate this algorithm in order to explore the trade-off between software optimizations, tightly coupled hardware acceleration by instruction set extension and modular, loosely coupled accelerators. We provide detailed results on the energy consumption and performance of our design and compare it to existing implementations of lattice- and code-based algorithms. The design was implemented in two technologies: FPGA and ASIC. Our results show that code-based algorithms are valid alternatives in low-power IoT from an implementation perspective.",
      "publication_date": "2023-01-12T09:05:06+00:00",
      "doi": "",
      "venue": "arXiv preprint",
      "citation_count": 0,
      "keywords": [
        "cs.CR"
      ],
      "arxiv_id": "2301.04888v1"
    },
    {
      "title": "FPGA-Placement via Quantum Annealing",
      "authors": [
        "Thore Gerlach",
        "Stefan Knipp",
        "David Biesner",
        "Stelios Emmanouilidis",
        "Klaus Hauber",
        "Nico Piatkowski"
      ],
      "abstract": "Field-Programmable Gate Arrays (FPGAs) have asserted themselves as vital assets in contemporary computing by offering adaptable, reconfigurable hardware platforms. FPGA-based accelerators incubate opportunities for breakthroughs in areas, such as real-time data processing, machine learning or cryptography -- to mention just a few. The procedure of placement -- determining the optimal spatial arrangement of functional blocks on an FPGA to minimize communication delays and enhance performance -- is an NP-hard problem, notably requiring sophisticated algorithms for proficient solutions. Clearly, improving the placement leads to a decreased resource utilization during the implementation phase. Adiabatic quantum computing (AQC), with its capability to traverse expansive solution spaces, has potential for addressing such combinatorial problems. In this paper, we re-formulate the placement problem as a series of so called quadratic unconstrained binary optimization (QUBO) problems which are subsequently solved via AQC. Our novel formulation facilitates a straight-forward integration of design constraints. Moreover, the size of the sub-problems can be conveniently adapted to the available hardware capabilities. Beside the sole proposal of a novel method, we ask whether contemporary quantum hardware is resilient enough to find placements for real-world-sized FPGAs. A numerical evaluation on a D-Wave Advantage 5.4 quantum annealer suggests that the answer is in the affirmative.",
      "publication_date": "2023-12-24T12:23:09+00:00",
      "doi": "",
      "venue": "arXiv preprint",
      "citation_count": 0,
      "keywords": [
        "quant-ph"
      ],
      "arxiv_id": "2312.15467v1"
    },
    {
      "title": "Energy-Efficient NTT Sampler for Kyber Benchmarked on FPGA",
      "authors": [
        "Paresh Baidya",
        "Rourab Paul",
        "Vikas Srivastava",
        "Sumit Kumar Debnath"
      ],
      "abstract": "Kyber is a lattice-based key encapsulation mechanism selected for standardization by the NIST Post-Quantum Cryptography (PQC) project. A critical component of Kyber's key generation process is the sampling of matrix elements from a uniform distribution over the ring Rq . This step is one of the most computationally intensive tasks in the scheme, significantly impacting performance in low-power embedded systems such as Internet of Things (IoT), wearable devices, wireless sensor networks (WSNs), smart cards, TPMs (Trusted Platform Modules), etc. Existing approaches to this sampling, notably conventional SampleNTT and Parse-SPDM3, rely on rejection sampling. Both algorithms require a large number of random bytes, which needs at least three SHAKE-128 squeezing steps per polynomial. As a result, it causes significant amount of latency and energy. In this work, we propose a novel and efficient sampling algorithm, namely Modified SampleNTT, which substantially educes the average number of bits required from SHAKE-128 to generate elements in Rq - achieving approximately a 33% reduction compared to conventional SampleNTT. Modified SampleNTT achieves 99.16% success in generating a complete polynomial using only two SHAKE-128 squeezes, outperforming both state-of-the-art methods, which never succeed in two squeezes of SHAKE-128. Furthermore, our algorithm maintains the same average rejection rate as existing techniques and passes all standard statistical tests for randomness quality. FPGA implementation on Artix-7 demonstrates a 33.14% reduction in energy, 33.32% lower latency, and 0.28% fewer slices compared to SampleNTT. Our results confirm that Modified SampleNTT is an efficient and practical alternative for uniform polynomial sampling in PQC schemes such as Kyber, especially for low-power security processors.",
      "publication_date": "2025-05-03T10:54:01+00:00",
      "doi": "",
      "venue": "arXiv preprint",
      "citation_count": 0,
      "keywords": [
        "cs.CR"
      ],
      "arxiv_id": "2505.01782v1"
    },
    {
      "title": "Optimizing CNN-based Hyperspectral Image Classification on FPGAs",
      "authors": [
        "Shuanglong Liu",
        "Ringo S. W. Chu",
        "Xiwei Wang",
        "Wayne Luk"
      ],
      "abstract": "Hyperspectral image (HSI) classification has been widely adopted in applications involving remote sensing imagery analysis which require high classification accuracy and real-time processing speed. Methods based on Convolutional neural networks (CNNs) have been proven to achieve state-of-the-art accuracy in classifying HSIs. However, CNN models are often too computationally intensive to achieve real-time response due to the high dimensional nature of HSI, compared to traditional methods such as Support Vector Machines (SVMs). Besides, previous CNN models used in HSI are not specially designed for efficient implementation on embedded devices such as FPGAs. This paper proposes a novel CNN-based algorithm for HSI classification which takes into account hardware efficiency. A customized architecture which enables the proposed algorithm to be mapped effectively onto FPGA resources is then proposed to support real-time on-board classification with low power consumption. Implementation results show that our proposed accelerator on a Xilinx Zynq 706 FPGA board achieves more than 70x faster than an Intel 8-core Xeon CPU and 3x faster than an NVIDIA GeForce 1080 GPU. Compared to previous SVM-based FPGA accelerators, we achieve comparable processing speed but provide a much higher classification accuracy.",
      "publication_date": "2019-06-27T22:05:22+00:00",
      "doi": "",
      "venue": "arXiv preprint",
      "citation_count": 0,
      "keywords": [
        "eess.IV"
      ],
      "arxiv_id": "1906.11834v1"
    },
    {
      "title": "Multi-Component Optimization and Efficient Deployment of Neural-Networks   on Resource-Constrained IoT Hardware",
      "authors": [
        "Bharath Sudharsan",
        "Dineshkumar Sundaram",
        "Pankesh Patel",
        "John G. Breslin",
        "Muhammad Intizar Ali",
        "Schahram Dustdar",
        "Albert Zomaya",
        "Rajiv Ranjan"
      ],
      "abstract": "The majority of IoT devices like smartwatches, smart plugs, HVAC controllers, etc., are powered by hardware with a constrained specification (low memory, clock speed and processor) which is insufficient to accommodate and execute large, high-quality models. On such resource-constrained devices, manufacturers still manage to provide attractive functionalities (to boost sales) by following the traditional approach of programming IoT devices/products to collect and transmit data (image, audio, sensor readings, etc.) to their cloud-based ML analytics platforms. For decades, this online approach has been facing issues such as compromised data streams, non-real-time analytics due to latency, bandwidth constraints, costly subscriptions, recent privacy issues raised by users and the GDPR guidelines, etc. In this paper, to enable ultra-fast and accurate AI-based offline analytics on resource-constrained IoT devices, we present an end-to-end multi-component model optimization sequence and open-source its implementation. Researchers and developers can use our optimization sequence to optimize high memory, computation demanding models in multiple aspects in order to produce small size, low latency, low-power consuming models that can comfortably fit and execute on resource-constrained hardware. The experimental results show that our optimization components can produce models that are; (i) 12.06 x times compressed; (ii) 0.13% to 0.27% more accurate; (iii) Orders of magnitude faster unit inference at 0.06 ms. Our optimization sequence is generic and can be applied to any state-of-the-art models trained for anomaly detection, predictive maintenance, robotics, voice recognition, and machine vision.",
      "publication_date": "2022-04-20T13:30:04+00:00",
      "doi": "",
      "venue": "arXiv preprint",
      "citation_count": 0,
      "keywords": [
        "cs.LG"
      ],
      "arxiv_id": "2204.10183v1"
    },
    {
      "title": "Hardware Accelerator and Neural Network Co-Optimization for   Ultra-Low-Power Audio Processing Devices",
      "authors": [
        "Christoph Gerum",
        "Adrian Frischknecht",
        "Tobias Hald",
        "Paul Palomero Bernardo",
        "Konstantin Lübeck",
        "Oliver Bringmann"
      ],
      "abstract": "The increasing spread of artificial neural networks does not stop at ultralow-power edge devices. However, these very often have high computational demand and require specialized hardware accelerators to ensure the design meets power and performance constraints. The manual optimization of neural networks along with the corresponding hardware accelerators can be very challenging. This paper presents HANNAH (Hardware Accelerator and Neural Network seArcH), a framework for automated and combined hardware/software co-design of deep neural networks and hardware accelerators for resource and power-constrained edge devices. The optimization approach uses an evolution-based search algorithm, a neural network template technique, and analytical KPI models for the configurable UltraTrail hardware accelerator template to find an optimized neural network and accelerator configuration. We demonstrate that HANNAH can find suitable neural networks with minimized power consumption and high accuracy for different audio classification tasks such as single-class wake word detection, multi-class keyword detection, and voice activity detection, which are superior to the related work.",
      "publication_date": "2022-09-08T13:29:09+00:00",
      "doi": "",
      "venue": "arXiv preprint",
      "citation_count": 0,
      "keywords": [
        "cs.SD"
      ],
      "arxiv_id": "2209.03807v2"
    },
    {
      "title": "Hardware-Software Co-Design of BIKE with HLS-Generated Accelerators",
      "authors": [
        "Gabriele Montanaro",
        "Andrea Galimberti",
        "Ernesto Colizzi",
        "Davide Zoni"
      ],
      "abstract": "In order to mitigate the security threat of quantum computers, NIST is undertaking a process to standardize post-quantum cryptosystems, aiming to assess their security and speed up their adoption in production scenarios. Several hardware and software implementations have been proposed for each candidate, while only a few target heterogeneous platforms featuring CPUs and FPGAs. This work presents a HW/SW co-design of BIKE for embedded platforms featuring both CPUs and small FPGAs and employs high-level synthesis (HLS) to timely deliver the hardware accelerators. In contrast to state-of-the-art solutions targeting performance-optimized HLS accelerators, the proposed solution targets the small FPGAs implemented in the heterogeneous platforms for embedded systems. Compared to the software-only execution of BIKE, the experimental results collected on the systems-on-chip of the entire Xilinx Zynq-7000 family highlight a performance speedup ranging from 1.37x, on Z-7010, to 2.78x, on Z-7020.",
      "publication_date": "2022-09-08T14:08:56+00:00",
      "doi": "10.1109/ICECS202256217.2022.9970992",
      "venue": "arXiv preprint",
      "citation_count": 0,
      "keywords": [
        "cs.AR"
      ],
      "arxiv_id": "2209.03830v2"
    },
    {
      "title": "SpikeExplorer: hardware-oriented Design Space Exploration for Spiking   Neural Networks on FPGA",
      "authors": [
        "Dario Padovano",
        "Alessio Carpegna",
        "Alessandro Savino",
        "Stefano Di Carlo"
      ],
      "abstract": "One of today's main concerns is to bring Artificial Intelligence power to embedded systems for edge applications. The hardware resources and power consumption required by state-of-the-art models are incompatible with the constrained environments observed in edge systems, such as IoT nodes and wearable devices. Spiking Neural Networks (SNNs) can represent a solution in this sense: inspired by neuroscience, they reach unparalleled power and resource efficiency when run on dedicated hardware accelerators. However, when designing such accelerators, the amount of choices that can be taken is huge. This paper presents SpikExplorer, a modular and flexible Python tool for hardware-oriented Automatic Design Space Exploration to automate the configuration of FPGA accelerators for SNNs. Using Bayesian optimizations, SpikerExplorer enables hardware-centric multi-objective optimization, supporting factors such as accuracy, area, latency, power, and various combinations during the exploration process. The tool searches the optimal network architecture, neuron model, and internal and training parameters, trying to reach the desired constraints imposed by the user. It allows for a straightforward network configuration, providing the full set of explored points for the user to pick the trade-off that best fits the needs. The potential of SpikExplorer is showcased using three benchmark datasets. It reaches 95.8% accuracy on the MNIST dataset, with a power consumption of 180mW/image and a latency of 0.12 ms/image, making it a powerful tool for automatically optimizing SNNs.",
      "publication_date": "2024-04-04T17:53:08+00:00",
      "doi": "10.3390/electronics13091744",
      "venue": "arXiv preprint",
      "citation_count": 0,
      "keywords": [
        "cs.NE"
      ],
      "arxiv_id": "2404.03714v1"
    },
    {
      "title": "LW-GCN: A Lightweight FPGA-based Graph Convolutional Network Accelerator",
      "authors": [
        "Zhuofu Tao",
        "Chen Wu",
        "Yuan Liang",
        "Lei He"
      ],
      "abstract": "Graph convolutional networks (GCNs) have been introduced to effectively process non-euclidean graph data. However, GCNs incur large amounts of irregularity in computation and memory access, which prevents efficient use of traditional neural network accelerators. Moreover, existing dedicated GCN accelerators demand high memory volumes and are difficult to implement onto resource limited edge devices. In this work, we propose LW-GCN, a lightweight FPGA-based accelerator with a software-hardware co-designed process to tackle irregularity in computation and memory access in GCN inference. LW-GCN decomposes the main GCN operations into sparse-dense matrix multiplication (SDMM) and dense matrix multiplication (DMM). We propose a novel compression format to balance workload across PEs and prevent data hazards. Moreover, we apply data quantization and workload tiling, and map both SDMM and DMM of GCN inference onto a uniform architecture on resource limited hardware. Evaluation on GCN and GraphSAGE are performed on Xilinx Kintex-7 FPGA with three popular datasets. Compared to existing CPU, GPU, and state-of-the-art FPGA-based accelerator, LW-GCN reduces latency by up to 60x, 12x and 1.7x and increases power efficiency by up to 912x., 511x and 3.87x, respectively. Furthermore, compared with NVIDIA's latest edge GPU Jetson Xavier NX, LW-GCN achieves speedup and energy savings of 32x and 84x, respectively.",
      "publication_date": "2021-11-04T22:29:53+00:00",
      "doi": "",
      "venue": "arXiv preprint",
      "citation_count": 0,
      "keywords": [
        "cs.LG"
      ],
      "arxiv_id": "2111.03184v1"
    },
    {
      "title": "Compiling Deep Learning Models for Custom Hardware Accelerators",
      "authors": [
        "Andre Xian Ming Chang",
        "Aliasger Zaidy",
        "Vinayak Gokhale",
        "Eugenio Culurciello"
      ],
      "abstract": "Convolutional neural networks (CNNs) are the core of most state-of-the-art deep learning algorithms specialized for object detection and classification. CNNs are both computationally complex and embarrassingly parallel. Two properties that leave room for potential software and hardware optimizations for embedded systems. Given a programmable hardware accelerator with a CNN oriented custom instructions set, the compiler's task is to exploit the hardware's full potential, while abiding with the hardware constraints and maintaining generality to run different CNN models with varying workload properties. Snowflake is an efficient and scalable hardware accelerator implemented on programmable logic devices. It implements a control pipeline for a custom instruction set. The goal of this paper is to present Snowflake's compiler that generates machine level instructions from Torch7 model description files. The main software design points explored in this work are: model structure parsing, CNN workload breakdown, loop rearrangement for memory bandwidth optimizations and memory access balancing. The performance achieved by compiler generated instructions matches against hand optimized code for convolution layers. Generated instructions also efficiently execute AlexNet and ResNet18 inference on Snowflake. Snowflake with $256$ processing units was synthesized on Xilinx's Zynq XC7Z045 FPGA. At $250$ MHz, AlexNet achieved in $93.6$ frames/s and $1.2$ GB/s of off-chip memory bandwidth, and $21.4$ frames/s and $2.2$ GB/s for ResNet18. Total on-chip power is $5$ W.",
      "publication_date": "2017-08-01T01:01:18+00:00",
      "doi": "",
      "venue": "arXiv preprint",
      "citation_count": 0,
      "keywords": [
        "cs.DC"
      ],
      "arxiv_id": "1708.00117v2"
    },
    {
      "title": "ONNX-to-Hardware Design Flow for the Generation of Adaptive   Neural-Network Accelerators on FPGAs",
      "authors": [
        "Federico Manca",
        "Francesco Ratto"
      ],
      "abstract": "Neural Networks (NN) provide a solid and reliable way of executing different types of applications, ranging from speech recognition to medical diagnosis, speeding up onerous and long workloads. The challenges involved in their implementation at the edge include providing diversity, flexibility, and sustainability. That implies, for instance, supporting evolving applications and algorithms energy-efficiently. Using hardware or software accelerators can deliver fast and efficient computation of the \\acp{nn}, while flexibility can be exploited to support long-term adaptivity. Nonetheless, handcrafting an NN for a specific device, despite the possibility of leading to an optimal solution, takes time and experience, and that's why frameworks for hardware accelerators are being developed. This work-in-progress study focuses on exploring the possibility of combining the toolchain proposed by Ratto et al., which has the distinctive ability to favor adaptivity, with approximate computing. The goal will be to allow lightweight adaptable NN inference on FPGAs at the edge. Before that, the work presents a detailed review of established frameworks that adopt a similar streaming architecture for future comparison.",
      "publication_date": "2023-09-23T09:41:43+00:00",
      "doi": "",
      "venue": "arXiv preprint",
      "citation_count": 0,
      "keywords": [
        "cs.AR"
      ],
      "arxiv_id": "2309.13321v1"
    }
  ]
}