Automated Concept Extraction and Hierarchical Concept Mapping from Research Papers

Introduction and Problem Overview

Researchers face an ever-growing volume of literature, making it challenging to stay current on all relevant concepts and developments. To alleviate this, we propose an automated pipeline that extracts key concepts from a collection of research papers and organizes them into a hierarchical concept map. Each node in this concept map represents a concept (topic, method, idea, etc.), with higher-level nodes grouping related sub-concepts. The map can be visualized interactively (e.g. via D3.js) to allow zooming into finer-grained topics, with node size or color indicating the number of papers supporting that concept. Clicking a node would reveal a description of the concept along with evidence sentences drawn directly from the papers (and links to those papers), to maintain traceability and trust.

Goals: The primary goal is to produce an accurate concept hierarchy that faithfully represents the themes in the paper collection. Equally important are explainability and reproducibility: domain experts (academics) must be able to inspect how the concepts were identified and organized, and verify that each concept is grounded in the source texts. This means favoring methods that are transparent (or can provide supporting evidence) and deterministic. We also seek a solution that is robust (works across different domains or subfields) and repeatable with minimal manual tuning, so that new caches of papers can be processed in the same way.

Scope: Our focus is on the NLP pipeline for concept mining from the papers. This includes text processing, concept extraction, concept deduplication and hierarchy induction, and linking concepts to evidence in the text. The visualization (e.g. using D3.js) is an important application of the output, but the core challenge is building the structured concept map data behind it. In what follows, we survey existing approaches and tools for concept extraction and taxonomy construction, then outline a detailed design for a system that “steel-mans” the idea – incorporating the best elements of these approaches to meet our goals.

Existing Work and Tools for Concept Extraction

A number of research efforts and open-source tools are directly relevant to this problem:
	•	Keyphrase/Concept Extraction Methods: Traditional unsupervised keyphrase extraction techniques provide a starting point. For example, statistical methods like TF–IDF and YAKE identify important terms by frequency and specialized scoring. Graph-based methods like TextRank represent the text as a word graph and use ranking algorithms (PageRank) to extract central terms ￼. Variants such as TopicRank cluster candidate phrases into topics before ranking, and PositionRank adds positional heuristics for weighting. These approaches are domain-agnostic and require no training data, which makes them robust and easy to reproduce. However, they typically extract only the “important” keywords of a single document, not all concepts, and they do not produce a hierarchy (the output is a flat list per document).
	•	Embedding-Based Techniques: Newer unsupervised methods leverage semantic embeddings to improve concept extraction. For instance, EmbedRank generates candidate phrases (e.g. via part-of-speech patterns) and embeds them in a vector space, then ranks them by cosine similarity to the document’s embedding. This helps to find terms that are semantically central to the document’s content, beyond what frequency alone would capture. Other tools like KeyBERT use BERT sentence embeddings to directly find the phrases most similar to the document meaning. These embedding methods often yield more meaningful phrases and can capture multi-word concepts. They remain extractive (choosing phrases present in text), preserving traceability. The challenge is that embedding models are a bit of a “black box,” but they are pre-trained on large corpora and widely tested, and their use of vector math (cosine similarity) is explainable to an extent. In our context, we can use embedding-based extraction to ensure we gather semantically important concepts from each paper (not just frequent terms). An example from our own prototype is using a MiniLM sentence transformer to embed paper abstracts and cluster them into concept groups ￼.
	•	Large Language Model (LLM) Approaches: The latest research (e.g. ConExion by Norouzi et al., 2025) has demonstrated using large language models for concept extraction. These approaches prompt an LLM (like GPT-3.5 or similar) to output key concepts from a document. The advantage is that LLMs can sometimes understand context and pick out relevant concepts even if they are phrased differently or implied. ConExion showed that a simple prompted LLM can outperform many prior unsupervised methods on scientific keyphrase benchmarks. They focus on extracting present concepts (the extractive scenario, where the concept terms appear explicitly in the text), which aligns with our need for evidence traceability. However, a pure LLM approach may raise concerns for our users: it’s not as easily explainable how the LLM decided on concepts, and it could introduce hallucinations (concepts that aren’t actually in the text) if not carefully constrained. Still, LLMs could be used in a supporting role – for example, to suggest possible groupings or to generate descriptions for concepts – while keeping the final outputs grounded in the text.
	•	Domain Ontology-Based Classification: Rather than extracting concepts de novo, another strategy is to classify papers against a pre-existing taxonomy of concepts. A notable example is the CSO Classifier, which maps research papers to topics in the Computer Science Ontology (CSO), a comprehensive expert-created taxonomy of CS research areas. The CSO Classifier reads the title, abstract, and keywords of a paper and returns a set of relevant CSO topics. Under the hood, it uses a combination of string matching, embedding similarity, and spreading activation on the ontology graph. The advantage here is that the output is already hierarchical (since CSO is a hierarchy of research areas) and highly interpretable (each topic has a defined meaning). For example, given a cache of AI papers, it might label them with concepts like “machine learning”, “reinforcement learning”, “deep neural networks”, etc., which are nodes in the ontology with parent-child relations (e.g. “machine learning” > “reinforcement learning”). This approach is explainable (it’s essentially an information retrieval task on known terms) and reproducible. The downside is that it relies on the existence of a curated ontology in the domain; for fields outside CS, we may not have an equivalent ontology. Nonetheless, if our research areas align with well-defined taxonomies (e.g. medical Subject Headings for biomedical papers, ACM CCS for computing, etc.), leveraging them can greatly improve accuracy and user trust.
	•	Concept Map Extraction (Relations between Concepts): Beyond just listing concepts, some works focus on extracting concept maps or knowledge graphs from text. Galletti et al. (2024) propose a pipeline for Automated Concept Map Extraction from educational texts. Their neuro-symbolic pipeline includes steps for summarization (to handle large input), candidate concept extraction (noun phrase extraction, etc.), and relation extraction to connect concepts into a graph. They use a pre-trained relation extraction model (REBEL) to identify relationships like “X is a type of Y” or “X causes Y” between the noun phrase concepts, and they also use a knowledge base (DBpedia) to ground/unify the concepts. The result is a graph where nodes are concepts and edges are labeled relations (e.g. “hasPart”, “usedFor”). This is slightly different from our hierarchy goal – a concept map can be a general directed graph, not necessarily a tree – but it’s related. It demonstrates how combining symbolic techniques (like knowledge base lookups and explicit relation rules) with semantic techniques (like neural relation extraction and embeddings) can yield a rich, explainable structure. For our purposes, we might not need all the arbitrary relation types; however, one relation of special interest is the hierarchical (is-a or part-of) relation – identifying when one concept is a sub-category or part of another. The concept map literature provides ideas for extracting such relations from text (for example, using Hearst patterns like “X is a kind of Y” to detect hypernym relationships).
	•	Recent Research on Enhanced Concept Extraction: The field is active, and novel approaches continue to emerge. For instance, Huang et al. (2023) introduce WERECE (Word Embedding Refinement for Concept Extraction), an unsupervised method that adapts pre-trained embeddings to better fit a target domain. They integrate manifold learning and clustering to improve precision in extracting educational domain concepts, significantly outperforming basic methods like TextRank or TF-IDF. Their approach underscores that some form of domain adaptation or refinement can boost accuracy – a consideration if we find vanilla embedding models lacking for niche research areas. Additionally, researchers have explored hierarchical topic models (like hLDA or more recent neural variants) that directly infer topic hierarchies from a corpus. Tools like BERTopic provide a practical implementation: it first finds topics using clustering of document embeddings and representative terms, then applies hierarchical clustering on the topics to form a tree of topics. BERTopic’s hierarchy is constructed by merging similar topics and recalculating representations, which could align well with our need to identify broad vs. narrow concept groupings.

In summary, there is a rich toolbox to draw from: rule-based and ontology-driven methods for precision and interpretability, statistical and graph algorithms for unsupervised keyphrase extraction ￼, embedding and clustering techniques for capturing semantic similarity, and even LLM-powered strategies for more comprehensive concept identification. The optimal solution will likely hybridize these approaches to capitalize on their strengths while mitigating weaknesses.

Methodologies for Concept Extraction

This section dives deeper into the techniques for extracting concept candidates from papers. We consider several approach categories, discussing how each works and its pros/cons in the context of building an accurate, explainable concept map.

1. Rule-Based and Knowledge-Based Extraction

These approaches rely on human-defined patterns or existing knowledge bases/ontologies:
	•	Rule-Based (Symbolic) Methods: These use linguistic rules or patterns to identify concepts. A simple example is extracting all noun phrases (NPs) from the text, since most key concepts in academic papers are noun phrases (e.g., “convolutional neural network”, “quantum entanglement”). Tools like spaCy or NLTK can do NP chunking out-of-the-box. More refined rule-based methods might apply domain-specific filters (e.g., ignore generic words like “approach”, “system” as standalone concepts, or enforce that a concept phrase contains at least one technical adjective or acronym). Rule-based extraction is fast and transparent – one can literally list the patterns being used. However, its recall can be limited (it might miss unconventional phrasing) and precision may suffer without extensive tweaking (many NPs are not actually meaningful concepts on their own). It also does nothing to unify synonyms or variants; “neural network” vs “neural networks” vs “artificial neural network” would all appear separately unless we add normalization rules.
	•	Dictionary- or Ontology-Based: If we have a list of known relevant concepts (a dictionary or an ontology of terms), we can simply scan the text for occurrences of those terms. For example, if analyzing biomedical papers, one could use the UMLS thesaurus to tag occurrences of medical concepts. Or for computer science, use the CSO terms. This approach is high precision – when a match is found, we know exactly what it refers to – but can be low recall if the dictionary is incomplete or if authors use new terms not in it. It also inherits any bias of the source ontology. Nonetheless, as mentioned, the CSO Classifier successfully uses an ontology-driven approach to detect research topics in papers. They combine it with a syntactic expansion step (to catch synonyms or hyponyms of ontology terms) and a contextual similarity step (to verify the paper’s text context matches the candidate topic). For our system, if a relevant ontology exists, we could incorporate it to either validate extracted concepts (e.g., flag if a found term is in the known list of domain concepts) or to provide structure (e.g., use ontology relations as edges in our concept hierarchy). However, academic research often ventures beyond established taxonomies; so while this can anchor well-known concepts, we’ll still need to handle novel or domain-specific terms.
	•	Hearst Pattern-Based Hypernym Extraction: A specific form of rule-based method to build hierarchies is using lexical patterns (first introduced by Hearst, 1992) to find “is-a” relationships in text. For instance, sentences like “X is a Y” or “Y such as X and Z” can indicate Y is a broader category and X, Z are examples (subtypes). Applying these patterns to a corpus of papers could automatically yield a set of candidate parent-child concept relations. This is interpretable (the evidence for the relation is literally the sentence matched by the pattern) and has been used in ontology induction research. Its drawback is that not all concept relations are stated in such explicit ways in research papers – authors often assume the reader knows the hierarchy and may not write those defining sentences. Also, parsing has to be accurate to avoid false matches. Still, if available, we can use patterns as an extra source of hierarchical edges to corroborate our other clustering-based hierarchy.

2. Statistical and Graph-Based Methods

These treat the text analytically, without heavy semantic modeling:
	•	Term Frequency / TF-IDF: Simply counting word or phrase frequencies in the corpus (and maybe weighting by inverse document frequency to downrank very common terms) can highlight which terms are significant in each document and across the domain. Many early keyword extraction methods rely on variants of this. TF-IDF can be effective for single-word terms or very domain-specific jargon that appears frequently ￼. However, it tends to favor very frequent words (which might be too general like “algorithm” if many papers use it) and might miss important but infrequent concepts (e.g., a seminal concept mentioned in only one paper). Also, it doesn’t inherently group synonyms (e.g., “CNN” vs “convolutional neural network” would appear as separate tokens). Despite these issues, it’s often a good baseline to include – for example, one might ensure that any term that is extremely frequent in the corpus is included in the concept list (as it likely represents a core topic of the collection).
	•	Keyword Graph and TextRank: As referenced earlier, TextRank is an unsupervised algorithm that builds a graph of words connected by co-occurrence (within a window in the text), then runs a ranking algorithm to score nodes (words) by centrality ￼. Usually it’s applied to single documents to get keyphrases. An extension, TopicRank, clusters similar words/phrases first, then ranks clusters – this is useful to avoid redundancy and to represent a broader concept by multiple terms. Graph-based methods like this are appealing because they consider the contextual network of terms – a word that co-occurs with many other high-importance words will get a higher score ￼. They are fully unsupervised and easily reproducible. For explainability, one can inspect the graph to see why a term was ranked highly (e.g. it’s connected to many others). In practice for our use case, a graph method could be used to extract initial candidates from each paper (or from the whole corpus) by treating each paper’s text as a small graph. It won’t give a hierarchy, but it can enrich the pool of candidate concepts. We must be careful, though, because graph methods often yield common phrases that are somewhat generic (depending on window size and other parameters) – they might need filtering.
	•	Statistical Topic Modeling (LDA): Latent Dirichlet Allocation (LDA) is a generative model that discovers “topics” in a collection of documents, where each topic is a probability distribution over words. Running LDA on our corpus could yield topics that roughly correspond to subdomains or themes. For example, one topic might heavily weight words {network, neural, learning, deep, model}, which we can label as “deep neural networks”, while another topic weights {theorem, proof, graph, conjecture} perhaps “theoretical computer science”. Basic LDA yields a flat set of topics. There are hierarchical variants like hLDA that arrange topics in a tree by introducing a nested Chinese Restaurant Process prior. LDA’s advantages: it’s unsupervised and has a solid theoretical footing; it tends to produce coherent groups of terms, which can be interpreted by humans as concepts. It is also relatively explainable: each topic is defined by a small set of highly weighted terms, which can serve as a description. And it’s reproducible if the random seed is fixed. However, LDA has some downsides for us: it treats topics as probability distributions rather than explicit phrases, so to get a concise concept label one often has to manually interpret the top words. Also, LDA assumes a generative model that may not fit well if our corpus is small or if papers are very heterogeneous. It might mix unrelated things or split a single real concept into multiple topics. Hyperparameter tuning (e.g. number of topics) is needed and not always obvious. Nonetheless, LDA could be used in combination with other methods – e.g. the set of top words from each topic can suggest concept groupings, or it could help determine a broad categorization of papers which we then refine with phrase extraction.

3. Embedding and Clustering Methods

These methods leverage vector representations of text to capture semantics:
	•	Document Embedding Clustering (Concept Areas): A straightforward approach used in our initial prototype is to represent each paper (perhaps by its abstract and title combined) as an embedding, and then cluster these embeddings to find groups of papers that are about similar topics ￼ ￼. Each such cluster can be considered a coarse-grained concept area – essentially an emergent category from the corpus. For example, if you have 100 AI papers, an embedding clustering might separate them into 3 clusters that roughly correspond to “computer vision”, “natural language processing”, and “robotics” (for instance). In our script, we used a simple hierarchical clustering with a cosine similarity threshold to group papers ￼ ￼. The cluster itself was then given a name by looking at common important words in the titles of papers in that cluster ￼ ￼. This is a quick form of concept labeling – e.g. if many titles in the cluster contain “neural network” or “deep learning”, those words end up in the common word list and form the cluster name like “Neural Network Learning” ￼. The benefits of document-level clustering are: (1) It directly addresses the concept areas associated with each paper (since each paper falls into one cluster or another), ensuring every paper is represented. (2) It’s fairly explainable: you can examine which papers grouped together and see their commonalities, and the threshold ensures a known level of similarity. (3) It naturally provides a count of papers per cluster (for node sizing). The drawback is that these clusters are broad and may mix multiple finer concepts if the threshold is loose; if the threshold is tight, you get many small clusters which might correspond to very specific concepts, but then organizing them hierarchically requires another step. Ideally, one can do multi-level clustering: first cluster at a high level to get broad domains, then cluster each cluster’s papers further to get subtopics, and so on (this essentially builds a hierarchical tree of clusters). Many libraries (like SciPy or BERTopic) can produce a dendrogram of clusters. We would need a strategy to cut that dendrogram at appropriate levels to form a manageable hierarchy (perhaps decided by similarity distance or by a desired number of top-level categories, etc.). Importantly, once we have clusters of papers, we can derive concept nodes representing each cluster. Each cluster’s “concept” can be summarized by: a name (from top frequent or TF-IDF words, or a keyphrase that appears often in that cluster’s papers), a description (maybe listing a few distinguishing keywords or a sentence), and the list of member papers. This is similar to the ConceptCluster in our code which generated a description from common keywords and counted papers ￼ ￼.
	•	Term or Phrase Embedding Clustering: Instead of clustering documents, we can attempt to directly cluster the extracted candidate terms/phrases themselves. For example, if across the corpus we extracted 500 unique noun phrases that look like potential concepts, we can embed each phrase (using a phrase embedding model or averaging word embeddings) and then cluster those. This way, synonyms or very closely related concepts (which should be unified) ideally end up in the same cluster. For instance, “convolutional neural network” and “CNN” would likely have very similar embeddings and cluster together. Each cluster of terms could then be merged into a single concept node in our map. This addresses the duplication issue (no duplicate concepts) by consolidating different mentions of the same underlying concept. It also can surface a hierarchy: if we do hierarchical clustering on terms, we’ll get a tree where each branch could be interpreted as a broader category containing narrower ones. However, interpreting clusters of short phrases can be tricky – we’d need a way to name those clusters (perhaps using the most central phrase or an external knowledge base lookup). One approach is iterative: first cluster very tightly to merge pure synonyms (basically cleaning the list of concepts), then cluster the cleaned list at a higher level to form categories. The c-TF-IDF technique in BERTopic is another way to represent a set of documents or words for clustering – it essentially creates a “topic vector” by combining words’ TF-IDF weighted in that subset. That could be applied to group similar concepts.
	•	Hybrid: Embeddings + Keywords: Some systems combine statistical and embedding approaches to get the best of both. For example, one could generate a large set of candidate phrases using a simpler method (like NP extraction + TFIDF filtering), and then use embeddings to refine that set (group them or remove those that are outliers semantically). The WERECE method does something along these lines: it refines a pre-trained word embedding space using manifold learning to better separate domain-specific terms, and then uses clustering and a discriminant function to pick out actual concept terms. The result was highly accurate for their domain, showing that attention to embedding space (to ensure domain terms aren’t drowning among general words) is important for accuracy. In our case, if we find that a general model’s embeddings aren’t distinguishing some concepts well (e.g., common English words vs technical usage), we could consider fine-tuning embeddings on our corpus or using a model specialized for scientific text (like SciBERT or Specter for papers).

Selecting an Embedding Model: Given explainability concerns, it might be better to use a well-known, smaller model like Sentence-BERT (all-MiniLM-L6-v2) which we already used ￼, or a domain-specific model if available (e.g., SciBERT for scientific text). These models are open and their behavior is somewhat understood; using them is a deterministic transformation (embed -> cluster), which we can justify to users as “grouping by semantic similarity”. We should avoid models that are too large and black-box (like GPT-4 embeddings, since that’s not easily self-hostable or inspectable by users). The Xenova transformer (used in the script) runs in-browser and is open source, which is a plus in terms of users trusting the method.

4. Supervised Learning Approaches

While our inclination is to avoid needing training data (for easier reproducibility), it’s worth noting what supervised methods exist, as they inform our choices:
	•	Sequence Labeling (NER-style): Some research treats concept extraction as a tagging problem – label each word in the text as part of a concept phrase or not (like identifying named entities). Brack et al. (2020) created a multi-domain annotated corpus of scientific abstracts with concept labels and trained BiLSTM-CRF and BERT models to extract concepts, achieving fairly high F1. Supervised models can incorporate a lot of linguistic knowledge and can in theory be very accurate at identifying concept boundaries and excluding non-concepts. But they need labeled data, which for arbitrary research domains is unavailable (and expensive to create). Also, if we present this to skeptical academics, a complex neural model that they can’t parse might reduce trust – unless it’s pre-trained on general data and fine-tuned on a public dataset, which they could inspect. We likely will not pursue training a custom model, but we can take cues: these models often leverage the fact that concepts are often noun phrases or specific technical terms, and they use contextual cues to decide if something is a concept mention or just a common word. We will replicate that logic with rules and unsupervised means as much as possible (e.g., focusing on nouns, capitalized terms, etc., and requiring a certain specificity).
	•	Classification (Topic Assignment): Another supervised angle is classifying whole documents into known categories (which is essentially what the ontology-based approach does in an unsupervised way). There are classifiers like SVMs or BERT-based classifiers that could be trained to assign labels (if one had a training set of papers labeled with concepts). This is not feasible without a pre-existing labeled dataset for our domains of interest. So we’ll skip this, preferring the unsupervised extraction and clustering.

In summary, unsupervised methods (from simple statistical to advanced embedding techniques) are favored for our scenario due to their domain independence and reproducibility. We will ensure all identified concepts are present in the text (no hallucinated abstractive concepts) so that we can trace them back to evidence. The balance of methods might look like: use rule-based and statistical methods to gather a broad net of candidate terms, use embedding clustering to merge and organize these terms, and optionally use knowledge-based filters to improve precision (e.g., drop obviously irrelevant or too-general terms, possibly with a stoplist or ontology check).

Constructing a Hierarchical Concept Structure

Once we have a set of concepts (likely a large set, possibly hundreds given “hundreds of PDFs”), the next challenge is to arrange them into a meaningful hierarchy or graph. The goal is to reflect relationships like broader vs narrower concepts, which will form the parent-child links in our concept map and enable the zoomable visualization. We discuss several strategies to achieve this structure:

Hierarchical Clustering and Topic Hierarchies

One natural approach is to apply hierarchical clustering to group concepts (or their associated documents) at multiple levels of granularity. This can be done in two main ways:
	•	Top-Down (Divisive) Clustering: Start with all documents or concepts in one cluster (representing the whole domain), then recursively split clusters. For example, perform a clustering to break the corpus into, say, k top-level clusters (with k chosen based on a target granularity or via a statistical cutoff), then take each cluster and cluster it further into subclusters, and so on. This could use the same algorithm at each level (e.g., k-means or agglomerative) but constrained within a cluster’s members. The advantages of top-down: it ensures a clean tree (no overlaps) and one can control how deep to go by setting stopping criteria (e.g., stop splitting when a cluster has fewer than N items or the items are too similar to each other beyond a threshold). It can also be guided by domain knowledge (e.g., specify the top-level division if known, such as “AI vs Systems papers” if it’s a mixed collection). Disadvantages: early mistakes (a wrong split at a high level) propagate downward. Also, divisive methods are less common in readily available libraries (though one can simulate by repeated clustering).
	•	Bottom-Up (Agglomerative) Clustering: This is more common – each document or concept starts as its own cluster, then clusters are merged step by step based on similarity until everything is in one cluster. This produces a full binary tree (dendrogram). We can then cut this tree at different levels to form a hierarchy. For instance, cut at a distance threshold to get top-level groups; within each group’s subtree, cut at a lower threshold for subgroups, etc. Tools like SciPy’s hierarchy.linkage (Ward method or others) can do this easily. Advantage: we don’t need to decide k at each level, we just decide distance thresholds (or number of levels). We get a nested structure naturally. Challenge: deciding where to cut is somewhat arbitrary – we might need to experiment or even allow the user to zoom continuously along the dendrogram. For visualization, we probably want a fixed number of discrete levels though (e.g., level 1 broad areas, level 2 sub-areas, level 3 specific topics, level 4 atomic concepts). We can achieve that by analyzing the cluster distances: often there are “gaps” in distance where merging very dissimilar clusters happens – those gaps can indicate natural cut points.

Regardless of top-down or bottom-up, using semantic similarity (embedding cosine or some distance) as the metric is key so that clusters reflect actual topical similarity. BERTopic’s approach, for example, effectively performs hierarchical clustering on topic vectors to suggest a topic merge tree. We can analogously cluster concept phrase embeddings.

One issue is labeling intermediate clusters (to name the parent nodes). Some approaches:
	•	Use the same method as before: find the most representative terms of that cluster. If our items being clustered are documents, we could recompute a mini topic model for that cluster or take the top TF-IDF terms in that subset ￼ ￼. If clustering terms, perhaps pick a term that is closest to the centroid as the label, or merge the words (though that might produce a weird concatenation).
	•	Use external knowledge: e.g., if a cluster contains “neural networks”, “SVM”, “deep learning”, we might label it “machine learning” because an ontology or Wikipedia category might identify that as the common parent. This could be done by mapping each term to a Wikipedia page and seeing if there is a common category or hypernym (but that can get complicated). Alternatively, we could feed the cluster’s items to an LLM and ask for a concise category name (but again, that’s a bit magical and might not be trusted by users unless it’s obviously correct).
	•	Simpler, we could present the cluster label as a list of top few terms rather than a single term. E.g., a node could be titled “{network, learning, algorithm}” or a short phrase like “network & learning algorithms” derived from those words. Our earlier script concatenated up to 3 frequent distinctive words capitalized as a name ￼ – not always grammatical, but it gives a hint. We could improve that with a template: e.g., “Algorithms for Neural Network Learning” if we detect one word is an umbrella (just an idea).

Hierarchical topic modeling (like hLDA or HLTA) is an alternative that directly yields a hierarchy of word clusters. These models ensure each child topic shares terms with the parent topic (creating a coherent tree). Using something like hLDA could be powerful: it might naturally output something like level 1: {Computer Vision, Machine Learning, …}, level 2 under ML: {Neural Networks, Probabilistic Models, …}, etc. However, hLDA is non-trivial to implement and tune, and not widely available in stable libraries compared to simpler clustering. It also might produce topics that aren’t explicitly present phrases. So practically, the clustering approach might suffice.

Taxonomy Induction and Parent-Child Relations

Beyond pure clustering, we can attempt to determine explicit parent-child links between concepts by analyzing their usage in text and their semantic inclusion:
	•	Inclusion Relations: If concept A appears very frequently whenever concept B appears (and maybe in text we see phrases like “A-based B” or “B using A”), it might suggest A is a sub-concept or a method under the topic B. For example, “neural network” appears in many “deep learning” papers, but “deep learning” might not be mentioned in some specific “neural network” papers that just assume it. We could measure co-occurrence statistics or conditional probabilities between concept terms across papers. A directed link A -> B (A is parent of B) might be hypothesized if: A appears in many papers without B, but whenever B appears, A often also appears or is implied. This is akin to subset frequency: concept B’s documents are largely a subset of concept A’s documents. One has to be careful: sometimes two concepts frequently co-occur because they are related siblings rather than parent-child. True hierarchical relations often involve hypernymy (is-a) which usually also manifests in definition sentences (which is where Hearst patterns help, as mentioned). If our corpus or their references contain glossary-like definitions, we might catch some (“X is a Y”).
	•	Knowledge Graph Approach: If some concepts can be linked to known entities (say via a tool like DBpedia Spotlight, which finds Wikipedia titles for mentions), we can query if Wikipedia or another ontology lists them in a hierarchy. Wikipedia categories or the “is a type of” from DBpedia could tell us that, say, Convolutional Neural Network is a subclass of Artificial Neural Network, which is a subclass of Machine Learning algorithm. Those could provide a chain of parents. Of course, not every concept will have a neat Wikipedia entry (especially newly coined ones in papers). This could be used opportunistically: integrate known relations for those we can map, and rely on clustering for the rest.
	•	Manual Curation Option: Since the target users are academics who might inspect the code, one approach is to allow some manual adjustments to the hierarchy. For example, the system could propose a hierarchy but allow a user to reorder or relabel nodes if they see fit, before finalizing the visualization. This would improve acceptance since they can inject their domain knowledge. However, the goal is minimal effort on their part, so ideally the automatic result is good enough. We should design for interpretability such that if they do check, they find the hierarchy logical.

Multi-Document Concept Relations

We should note that our hierarchy is primarily an organizational tool, not necessarily claiming a logical ontology. It’s fine if it’s somewhat subjective or only relevant to this corpus. The hierarchy edges in our concept map could simply mean “Concept B is a sub-topic within the scope of Concept A in this literature set.” This is a looser definition than a true is-a relationship in all contexts. For visualization and comprehension, that’s acceptable. We will make sure to provide evidence or rationale for these relations where possible (like maybe showing the cluster themes or a representative paper title that links the two concepts).

The output data structure we envisage will likely have a tree (or forest) of concept nodes. Each node may have properties: name, description, paperCount, parentId (except root nodes), children (list of sub-concepts), and evidence (some text/snippets). Additionally, since the user wants to click and get evidence, we might maintain a mapping from concept to supporting sentences.

Evidence Extraction and Concept Descriptions

For every concept in the map (from top-level down to atomic concepts), we want to attach human-readable explanations. There are two main forms of explanation we will provide:
	1.	Evidence Sentences/Paragraphs: These are direct quotes from the papers that mention the concept. Since we ensure concepts are extractive (present in text), finding evidence is usually as simple as searching for the concept phrase in the corpus. We can use the context around the first occurrence or the most illustrative occurrence. For example, if the concept is “transformer model”, an evidence sentence might be “We develop a transformer model that outperforms previous RNNs on XYZ task.” This shows usage of the concept. Ideally, we want explanatory evidence – perhaps a sentence that defines or elaborates the concept. Sometimes the first time a concept appears in a paper’s introduction, it might be introduced with some explanation (e.g. “X is a widely used technique for Y.”). We could implement a heuristic: when collecting evidence, favor sentences where the concept appears near phrases like “is a”, “refers to”, or within quotes or parentheses which might indicate a definition. We could also take 1–2 sentences of context around the term for clarity. Each evidence snippet will be stored along with a reference (paper ID and maybe section or paragraph number) so that the front-end can link to the exact PDF location (if we host the PDFs or direct to an external viewer). This evidentiary grounding is crucial for user trust – they can see exactly where each concept came from.
	2.	Concept Descriptions: Beyond raw quotes, it’s useful to have a concise description for each concept node. This could be a short summary (one or two sentences) of what that concept means in the context of this corpus. For atomic concepts, this might just paraphrase a definition from literature. For higher-level group nodes, it might say what themes that cluster covers. For example, a cluster concept “Deep Learning” might have a description like “Techniques involving deep neural networks and representation learning, appearing in 15 papers in this collection”. In our earlier code, we auto-generated a description of clusters by listing common keywords and the number of papers ￼ ￼ – e.g., “Research cluster containing 5 papers focusing on Convolutional Networks. Common themes: image recognition, CNN, deep learning.” That is already quite useful. We can refine this by possibly using a template or more informative metrics (maybe mention the earliest paper year among them if relevant, or the key authors if that cluster centers on a particular sub-community – though that might be too detailed). If we wanted, we could also use a language model in a constrained way: feed the list of keywords and ask it to produce a readable sentence. But this might not be necessary if simple string interpolation works. For instance, description = “Cluster of {N} papers about {cluster_name}, covering topics such as {top-3 keywords}.” is straightforward.

The concept descriptions can also be enriched with external info if available and not too “AI-magic”. For example, if a concept maps to a Wikipedia article, we could pull the first sentence of that article as a definition. That would give an authoritative description. The downside is it introduces an external dependency and might conflict with how the concept is used in our specific corpus context. Perhaps a balanced approach: provide the corpus-based description, and optionally a link to an external reference for general knowledge (maybe not needed if users themselves are domain experts).

In any case, these descriptions and evidence will be part of the output JSON so that the front-end can display them when a node is clicked. Ensuring every concept has at least one evidence sentence is a good practice – it’s the proof to the skeptical user. If a concept node is a cluster that didn’t explicitly appear as a phrase (like “Machine Learning” cluster might be named by us but perhaps each paper just mentioned specific algorithms and not the literal phrase “machine learning” often), we should still back it up. We could choose an evidence sentence from one of the papers in that cluster that implicitly indicates the cluster’s theme. Or we might list a couple of paper titles in that cluster to show examples. The user’s requirement was evidence sentences, presumably from the body of papers. So we will focus on that.

Linking to PDFs: Since the user has a repository of PDFs, we can host them or reference them by filename. A concept’s evidence sentence could have a link like “Paper XYZ (2021), p.3” which opens the PDF at page 3. This requires maybe converting PDFs to text and noting page numbers. Alternatively, if the PDFs are stored on GitHub, we can perhaps link to the PDF file directly (the browser will download or display it). Some PDF viewers support URL fragment to a page (like #page=3 for PDF in-browser). We’ll need to check how to implement that, but including a direct link to the PDF is straightforward.

Proposed System Pipeline (Step-by-Step)

Based on the above exploration, here is a detailed pipeline for implementing the concept extraction and mapping system. Each step includes technical details and justifications:

Step 1: Paper Text Extraction and Preparation
Input is a repository of PDF files (and possibly metadata like titles, authors if available). We will use a PDF parsing library (e.g., PyMuPDF or PDF.js or a CLI like pdftotext) to extract textual content from each PDF. Focus on sections with informative content (title, abstract, introduction, conclusion) – though concepts could appear anywhere, the abstract/introduction often contains high-level topics and definitions. We should retain the association of text with paper ID and maybe section. Also, gather metadata: title, authors, year, etc., either from the PDF or a provided metadata file (the user mentioned an output folder example, possibly with metadata.json as in the script ￼). Having titles and any author keywords is useful, because author-provided keywords are basically concepts by definition. We will include author keywords if available as guaranteed concepts for that paper (they can be fed into the extraction pipeline).

Perform basic NLP preprocessing: tokenize sentences (for evidence extraction later), and possibly normalize text (lowercasing for comparison, but keep original case for presentation). Remove obvious stopwords and punctuation when analyzing, but keep them in original sentences for display. If using specialized tools (like SciSpacy), we can also detect entities or UMLS concepts here, but let’s keep it simple initially.

Step 2: Candidate Concept Extraction per Paper
For each paper’s text, extract a set of candidate concept terms:
	•	Use noun phrase extraction to get multi-word candidates. SpaCy can give noun chunks; we filter them (drop those shorter than say 2 characters, drop pure stopword chunks, etc.). We may also include single nouns that seem domain-specific (determining that could be via a domain glossary or by checking if they are capitalized or an acronym). For scientific text, acronyms and proper nouns often signify key concepts (e.g. “BERT”, “ResNet”, “Moore’s Law”).
	•	Augment with any author keywords or section titles (if the paper has a “Keywords” section or if section headings themselves are domain terms).
	•	Calculate a salience score for each candidate: could be TF-IDF with respect to the corpus, or TextRank score within the doc, etc., to prioritize them. We might initially collect many candidates and then filter globally.
	•	One might also run EmbedRank/KeyBERT on the document: embed the document and each candidate phrase, and compute similarity to rank them. This could improve ordering (ensuring the candidates are actually relevant to the doc’s main topic, not just random frequent phrases). If computational resources allow, this is a good idea and uses the same embedding model we plan to use later.
	•	Limit each paper to, say, top 10-20 candidate concepts to avoid a blow-up of trivial terms. We assume hundreds of papers, 10 each gives a few thousand candidates total, which is manageable. The union of all candidates across papers is our initial concept pool. (This pool will still contain duplicates and synonyms across papers).

Step 3: Concept Candidate Consolidation (Global)
Now, take the pool of candidate concepts from all papers and merge duplicates and obvious variants:
	•	Normalize strings (lowercase, maybe singularize plurals if that makes sense, strip hyphens etc.). We should be careful: singular vs plural might matter (e.g. “neural network” vs “neural networks” – one is generic concept vs mention in context; we can treat them the same concept).
	•	If two candidates are identical after normalization, merge them. Also merge known synonyms/abbreviations if we can detect them. For abbreviation, if “CNN” and “convolutional neural network” both appear, and one is often in parentheses of the other, we can link them. A simple heuristic: if one phrase’s acronym matches another (we can generate acronym of a phrase easily and compare).
	•	Use embedding similarity among the candidate terms to catch closely related phrases. Cluster or group terms that have cosine similarity above a high threshold (e.g. >0.8). Within each group, pick a representative (perhaps the longest phrase or the one that appears most frequently in corpus). This representative will be our canonical name for that concept. The others can be stored as aliases (for searching evidence, we should recognize them). For example, group {“CNN”, “convolutional neural network”, “convnet”} into one concept node, named “Convolutional Neural Network (CNN)” perhaps.
	•	The result is a de-duplicated concept list.

Step 4: Build Initial Concept Hierarchy (Clustering)
With the unique concept list, perform hierarchical clustering to organize them. Two possible tracks:
	•	A: If we want to leverage document clustering, first cluster documents (papers) by embedding into broad domains, as described earlier. Then for each document cluster, look at the concepts (from Step 3) that occur in those papers and treat those as child concepts of that domain node. This naturally forms two-level hierarchy (domain -> concepts). We can further subcluster within each concept set if needed to create sub-subconcepts. This approach ties concepts directly to paper groups.
	•	B: Directly cluster concept embeddings hierarchically. This yields a multi-level taxonomy purely among concepts. This might be more domain-agnostic. However, one risk is that some very general concepts might cluster as children of multiple parents or somewhere ambiguous in a tree. Pure clustering will force a single tree. This is okay if semantically things mostly have one parent in context. For multi-disciplinary concepts, we might have to either allow duplicates or choose a primary parent (perhaps the one with which it had highest similarity).

We can actually combine A and B: use document clusters for top-level (ensuring each broad area corresponds to a set of papers), then within each area, cluster the concepts of that area for sub-levels. This way, we don’t accidentally cluster concepts from unrelated fields together just because they have a linguistic similarity (e.g. “Graph” could appear in ML and in “Graph theory” – embedding alone might cluster it with math concepts, but if our top-level separated AI vs Theory papers, we keep those contexts apart).

Technically, for clustering we use the cosine similarity on embeddings. The script used a fixed threshold method (single-link clustering: any paper above 0.7 similarity joins cluster) ￼. We could similarly pick thresholds for concept similarity to decide parent-child. Alternatively, use an algorithm like HDBSCAN (density-based) that can automatically determine clusters and allow outliers. HDBSCAN can also give a hierarchy of clusters as it varies its parameters.

For concreteness, suppose we identify 5 top-level clusters of papers after Step 4A (or by manual knowledge of subfields). For each, we create a node. Then within each, we cluster their concept terms to say 5 subclusters each (that yields ~25 second-level nodes if even). Those subclusters might still have many terms if the area is broad, so we can go one more level: cluster each subcluster’s terms further. We continue until either a cluster is small enough or cannot be meaningfully split. We need to define a stop criterion: e.g., stop when a cluster has fewer than M concepts or the average similarity of items in it is above some high threshold (meaning they are very coherent already). At the lowest level, you’ll have either individual concepts or small groups of very closely related concepts (which we might treat as essentially one concept with aliases, as we already did merging, so ideally lowest-level are single concepts).

It’s important to ensure the hierarchy isn’t too deep or too shallow. If too deep, the visualization might be cumbersome (though zooming helps). If too shallow (just one level of clusters), we miss the multi-level insight. Based on experience, 3 levels is often a sweet spot: high-level categories, mid-level topics, and specific concepts. We can adjust as needed.

Assigning Parent-Child Links:

When we cluster, we’ll have to create actual parent nodes (for each cluster) and link children (either individual concepts or subclusters). The dendrogram approach inherently creates parent nodes for each merge. In an output JSON, we might represent the tree recursively. Each cluster node can contain either subclusters or actual concept items as children. In our context, every node itself should be a concept (except perhaps a root “All Papers” node if we include one). If a cluster doesn’t correspond to an easily nameable concept, we might remove that level. For example, if we cluster concept embeddings, you get an arbitrary binary tree – but maybe only some merges correspond to meaningful categories. We could flatten any levels that seem artificial (like cluster just merging two single very close terms – we’d treat them as one concept anyway).

If using approach A (document clusters), then the top-level parent nodes are essentially domains (which might be generic, like “Computer Vision”); their children are actual concepts (like “object detection”, “image segmentation”). It might be good to allow a concept to appear under multiple domains if the concept is inherently cross-domain (for instance “neural network” might be in both CV and NLP clusters). However, that breaks a strict tree structure and becomes a DAG. Visualization of a DAG is more complex (concept node with multiple parents). We could duplicate the node in two branches for visualization simplicity, or choose the best fit. For now, perhaps stick to tree by assigning each concept one primary parent based on where it appears most.

Verification and Refinement:

After initial hierarchy is built, we can do some checks:
	•	Does every paper’s key topics appear somewhere? (We might ensure each paper has at least one concept leaf node that links to it, else we missed something.)
	•	Are there glaring duplicate concepts that didn’t merge? (We could search for overlap in names or high-similarity between sibling nodes to see if they should merge).
	•	If a known important concept from manual inspection is missing, why? Possibly our extraction failed – maybe it was a two-word phrase not in noun chunk form, etc. We can add rules to catch those (like specific domain terms known).
	•	For explainability, we might produce a log or report of how clusters were formed: e.g., list of concepts in each cluster, the similarity scores, etc., which a developer or an interested user could look at to verify.

Visualization Design Considerations (using D3.js)

While the question emphasizes that visualization is secondary, we should ensure our output supports it:
	•	We will output a JSON (or similar) structure with nested concepts. The metadata can include the number of papers under each node (for sizing) and maybe an importance score. Node coloring could reflect that or perhaps reflect level in hierarchy. We might choose color by top-level category (so all concepts in one domain share a color hue).
	•	We plan for a collapsible tree or radial zoomable sunburst chart. D3 supports both. A sunburst (radial partition) is great for showing proportion (if we weight by number of papers) and allows clicking to zoom into a segment which then becomes the full circle for the next level. A collapsible tree (like an indented tree or force-directed graph) might be more straightforward for showing concept names clearly and clicking to expand children.
	•	The user mentioned “zoom behavior to reveal different levels” – sunburst or zoomable treemap fit that description. We can adapt any D3 template for hierarchical data.
	•	On clicking a node, we will display a panel (perhaps a tooltip or side panel) with the concept’s description and evidence sentences. Each evidence sentence can be a clickable link. If we have the PDF accessible via URL, clicking could open it (possibly we can even deep-link to the exact page if known, e.g., paper.pdf#page=5). Alternatively, if we have an internal viewer, the link could trigger that (this is more on the front-end side).
	•	Ensure performance: hundreds of nodes are fine for D3. If it were thousands it might need some optimizations or lazy loading collapsed nodes (collapsing helps anyway).

Ensuring Explainability and Reproducibility

To make academics comfortable with this AI-aided tool, we will emphasize the following in our implementation and documentation:
	•	Transparent Algorithms: We use well-known techniques (clustering, embedding, keyword extraction) with open-source models. We can document each step in the repository (perhaps in a README or even inline code comments, similar to how the provided script had educational notes ￼ ￼). The logic of why a concept was included and why it is under a parent will be traceable (e.g., via similarity scores or explicit pattern matches).
	•	Parameter Justification: We will choose parameters (like similarity thresholds, number of clusters) based on either published recommendations or empirical tuning that we will describe. For instance, if we pick a 0.7 cosine threshold for clustering, we might note that this tends to group papers with very closely related abstracts ￼ ￼. We might even allow these thresholds to be easily adjustable by the user if they want to experiment.
	•	No Opaque AI Decisions without Evidence: Every concept we output is backed by at least one actual mention in the text (we avoid “absent keyphrases” or overly generalized terms not in text). And every grouping or relation we assert can be justified by either similarity (we could provide a representative similarity score) or a textual pattern. If using an LLM at any point (say we decide to generate a nicer description sentence), we will treat that as a helper that the user can verify (we’d still provide the raw evidence so they can double-check the description).
	•	Reproducibility: The code will run deterministically given the same input. We’ll fix random seeds for any algorithm that has randomness (e.g., initial centroid in k-means, or random tie-breaks in TextRank). This means if the academic runs it on their machine, they get the same concept map (unless new papers are added). We’ll also note the versions of libraries and models to ensure consistency (embedding models especially, since different versions might produce slightly different vectors).
	•	Modularity: Each step can be examined independently. For example, after candidate extraction, we could output the list of candidates per paper – the user can inspect those to see if they make sense. After concept merging, we could output the final list of unique concepts – they can scan if any obvious concept is missing or duplicated. The hierarchy can be output as an indented list to inspect structure without the UI. By breaking it down, a user can build trust at each stage rather than being confronted only with the final complicated output.

Example and Validation

To illustrate, imagine our corpus is a cache of 50 papers in machine learning. The process might yield something like this (hypothetical example):
	•	Top-Level Categories (Level 1): {“Computer Vision”, “Natural Language Processing”, “Theory”} – derived from document clustering (perhaps the abstracts clearly split into those themes). Suppose 20 papers in CV, 20 in NLP, 10 in Theory.
	•	Second Level (Level 2): Under “Computer Vision”, we might have concepts “Object Recognition”, “Image Segmentation”, “3D Vision”. Under “Natural Language Processing”: “Language Modeling”, “Machine Translation”, “Information Extraction”. Under “Theory”: maybe “Optimization Algorithms”, “Learning Theory”, “Probabilistic Modeling”.
	•	Third Level (Level 3, specific concepts): Under “Object Recognition”: we might list “Image classification”, “CNN (Convolutional Neural Network)”, “ImageNet dataset”, etc., with each being a leaf concept gleaned from papers. Some of these could arguably go another level (CNN could be considered a child of deep learning which spans CV and NLP, but in our tree we might put it under Object Recognition because most CV papers use CNNs).
	•	Each concept node would have paperCount – e.g., “CNN” might have 12 papers (because 12 of the CV papers mention CNNs). Node size reflects that.
	•	Clicking “CNN” might show description: “Convolutional Neural Network – a class of deep learning models commonly used for image recognition ￼ ￼. Present in 12 papers in this collection.”, and evidence such as: “…we design a convolutional neural network (CNN) for image classification achieving state-of-art accuracy…” (with a link to that paper).

Such a hierarchy provides a map: an academic could see that in their CV papers, CNNs and image classification are a big theme (large node), whereas maybe “GANs” (Generative Adversarial Networks) appear only in 2 papers (small node). If they zoom out, they see CV vs NLP as two big areas; if they zoom in, they see specific methods and datasets. This helps them quickly identify areas of concentration or gaps.

We also ensure that if the user questions “why is X under Y?”, we have an answer. For instance, if they ask why “ImageNet” (a dataset) is under “Object Recognition”, we could point out that the papers that mention ImageNet are all about image recognition tasks, and perhaps one paper explicitly says “…on the ImageNet object recognition benchmark…”. It might not be an is-a relationship universally (ImageNet is a dataset, not a subtype of object recognition), but in context it’s grouped there because that’s where it’s relevant.

Related Repositories and Papers for Further Reading

To further ground our approach in existing knowledge and allow implementation guidance, here are some key resources:
	•	ConExion: Concept Extraction with LLMs (2025) – Paper and code. Demonstrates how prompts and fine-tuned language models can extract keyphrases from scientific text, and compares unsupervised methods. Useful for understanding state-of-art unsupervised baselines and the importance of focusing on present (extractive) concepts. Source: Norouzi et al. 2025, code at GitHub ISE-FIZKarlsruhe/concept_extraction.
	•	CSO Classifier (2019) – Ontology-driven topic classifier for research papers. Open source on GitHub (angelosalatino/cso-classifier). It illustrates mapping text to a known taxonomy using a combination of string matching and embedding-based similarity. Even if one’s domain isn’t CS, the methodology can be adapted by swapping in a different ontology.
	•	Automated Concept Map Extraction Pipeline (2024) – Galletti et al.’s work on building concept maps from text. They provide an open-source pipeline combining summarization, ranking, entity extraction (noun chunks), and relation extraction (REBEL model). Their results show improvements by using neuro-symbolic techniques. This is a great reference for implementing relation extraction and using knowledge base linking (DBpedia) for concept grounding. While their focus is educational text and concept maps for learning, the techniques carry over to research papers.
	•	BERTopic Library – An open-source Python library by Grootendorst for topic modeling with BERT embeddings, including hierarchical topic modeling features. Even if we don’t use it directly, it provides methods for merging topics and visualizing them. It could be used out-of-the-box on our corpus to get an initial set of topics and hierarchy, which we can compare with our custom approach. BERTopic also has nice visualization integrations (it can output plots, though we might still build our own for full control).
	•	WERECE (2023) – Though domain-specific to education, this paper provides insight into refining word embeddings for concept extraction. If our concept extraction precision is low, their approach of using manifold learning to adapt embeddings and using a clustering-based discriminant might be something to borrow. They also compare against baseline algorithms like isolation forest for anomaly detection (treating domain terms as anomalies among general terms – an interesting angle).
	•	Hierarchical LDA (hLDA) – Classic paper by Griffiths, Tenenbaum, etc. (2004) on nested Chinese Restaurant Process for discovering topic hierarchies. And more recent Hierarchical Topic Modeling with Transformers (2022) that use neural networks to enforce topic trees. These are more researchy, but if interested, they discuss the difficulties in evaluating hierarchies and ensuring each child topic is a specialization of the parent – challenges we also face in an unsupervised setup.
	•	Keyphrase Extraction Surveys: To get a broader background, one can refer to surveys like the one by Banarescu et al. or others (as referenced in ConExion). They outline many techniques in the field of automatic keyphrase extraction and could offer additional ideas (like multipartite graphs, or using keyphrase position in text – e.g., introduction and conclusion often contain the important ones).

By synthesizing ideas from these resources, our approach stands on the shoulders of prior work but is tailored to our specific use case: building an interactive, evidence-backed concept map for research papers. We place heavy emphasis on accuracy (via clustering and consolidation to avoid missing or duplicating concepts), and on explainability (via extractive grounding and transparent methods), aligning with what domain experts expect.

Conclusion and Outlook

In this white paper, we outlined how to go from a raw collection of research papers to a rich, hierarchical concept map that can be visualized and explored. The proposed system uses a pipeline of NLP techniques – from simple (noun phrase extraction, TF-IDF) to advanced (transformer embeddings, hierarchical clustering) – to identify key concepts and organize them into a multi-level taxonomy. We ensure each concept is traceable to source text, addressing the trust issue head-on: users can verify any node in the map by examining evidence from the papers themselves. The approach balances unsupervised learning (for flexibility and minimal manual training) with opportunities to incorporate human knowledge (ontologies or minor manual adjustments) when available, yielding a solution that is both automated and credible.

This framework is designed to be robust and repeatable. Given a new set of papers, one can run the same pipeline to get an updated concept map. This makes it feasible for academics to continuously update their domain maps as new papers are published – a dynamic “living review” of the literature. Because the code and methodology are transparent, researchers can modify parts of the pipeline if they have domain-specific insights (for example, adding a custom list of stopwords or tuning the similarity threshold for what constitutes a distinct concept). The explainability means that, rather than blindly trusting an “AI”, users are invited to inspect and even collaborate with the system’s outputs, hopefully increasing their adoption comfort.

Future Enhancements: Once the basic system is in place, several improvements can be explored:
	•	Incorporating a feedback loop where users flag incorrect or missing concepts and the system adjusts (active learning style).
	•	Using the concept hierarchy to improve information retrieval – e.g., clicking a concept could show all sentences (or all papers) related to that concept, functioning as a guided literature review tool.
	•	Extending to relations between concepts beyond hierarchy, possibly integrating a relation extraction component to show, for instance, which methods are used for which tasks (this could appear as cross-links on the concept map, though that may clutter a bit).
	•	Evaluating the quality of the concept map quantitatively, if possible, by comparing to an expert-created classification or to the structure of survey articles in the field (some surveys explicitly list major subtopics – our system’s output could be benchmarked against that).
	•	User interface tweaks: e.g., search functionality within the concept map (type a concept name to highlight where it is in the hierarchy), or filtering (show me concepts that appear in at least 5 papers, etc.).

By implementing the system as described and leveraging existing open-source components, we can create an application that turns a repository of PDFs into a visual knowledge map. This will help researchers quickly grasp the landscape of their field, identify important themes and connections, and drill down to the supporting details when needed – all with minimal manual effort and maximum confidence in the results due to the system’s transparency and evidence-backed approach.

Overall, the strategy outlined here “steel-mans” the initial idea by combining the best practices from concept extraction research with practical considerations for user trust and interpretability. It acknowledges the complexity of language and domain knowledge in research papers, and addresses it with a mix of modern NLP and human-aligned design. The result will be a powerful tool in any researcher’s arsenal for staying up-to-date and making sense of vast literature terrains.

Sources:
	•	Norouzi et al. (2025). ConExion: Concept Extraction with Large Language Models – Presents an LLM-based approach to extract all present concepts from documents, and reviews unsupervised keyphrase extraction methods (TF-IDF, TextRank, TopicRank, EmbedRank, etc.) ￼. Emphasizes extractive concepts for ontology coverage.
	•	Galletti et al. (2025). Automated Concept Map Extraction from Text – Proposes a neuro-symbolic pipeline (summarization, ranking, entity extraction, relation extraction) for concept maps. Demonstrates combining semantic and symbolic techniques and reports improved performance with F1 ~25–28% (vs <20% in older methods).
	•	Huang et al. (2023). WERECE: Word Embedding Refinement for Educational Concept Extraction – Develops an unsupervised method that refines embeddings for domain concepts and clusters them, achieving high precision (~86% F1) and outperforming TextRank, TF-IDF, etc..
	•	Salatino et al. (2019). The CSO Classifier: Ontology-Driven Detection of Research Topics – An unsupervised tool mapping papers to the Computer Science Ontology. It returns research concepts drawn from a predefined taxonomy, improving over alternative topic extraction methods.
	•	Academic-Paper-Discovery (2025) – Internal project repository (scripts) demonstrating an embeddings-based concept map generation. Uses MiniLM embeddings to cluster research papers by similarity and names clusters via common title words ￼ ￼. Outputs a JSON concept map with clusters, relationships, and paper metadata ￼ ￼, illustrating a practical implementation of many discussed ideas.